<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[pandapower tutorial]]></title>
    <url>%2F2018%2F12%2F02%2Fpandapower-tutorial%2F</url>
    <content type="text"><![CDATA[新手指南4 简短介绍pandapower中的一个网络是由一个pandapowerNet对象表示的，该对象是由pandas的Dataframes的集合组成的。pandapowerNet中的每一个dataframe包含了每一个pandapower元件的信息，例如线路、负荷、变压器等等。 以一个简单的3节点示例网络作为一个小例子： 4.1 创建一个网络上面的3节点系统可以采用pandapower按下面代码生成： 1234567891011121314151617181920import pandapower as pp# create empty netnet = pp.create_empty_network()# create busesb1 = pp.create_bus(net, vn_kv=20., name="Bus 1")b2 = pp.create_bus(net, vn_kv=.4, name="Bus 2")b3 = pp.create_bus(net, vn_kv=.4, name="Bus 3")# create bus elementspp.create_ext_grid(net, bus=b1, vm_pu=1.02, name="Grid Connection")pp.create_load(net, bus=b3, p_kw=100, q_kvar=50, name="Load")# create branch elementstid = pp.create_transformer(net, hv_bus=b1, lv_bus=b2, std_type="0.4 MVA 20/0.4 kV", name="Trafo")pp.create_line(net, from_bus=b2, to_bus=b3, length_km=.1, name="Line", std_type="NAYY 4x50 SE")net This pandapower network includes the following parameter tables: - line (1 element) - bus (3 elements) - trafo (1 element) - ext_grid (1 element) - load (1 element) 需要强调的是你不用计算等效电路的任何阻抗或是分接比，pandapower内部通过变压器模型自动处理。标准类型库支持舒适地创建线路和变压器元件。 pandapower的表示形式如下： 1print(net.bus) name vn_kv type zone in_service 0 Bus 1 20.0 b None True 1 Bus 2 0.4 b None True 2 Bus 3 0.4 b None True 1print(net.trafo) name std_type hv_bus lv_bus sn_kva vn_hv_kv vn_lv_kv \ 0 Trafo 0.4 MVA 20/0.4 kV 0 1 400.0 20.0 0.4 vsc_percent vscr_percent pfe_kw ... tp_mid tp_min tp_max \ 0 6.0 1.425 1.35 ... 0 -2 2 tp_st_percent tp_st_degree tp_pos tp_phase_shifter parallel df \ 0 2.5 0.0 0 False 1 1.0 in_service 0 True [1 rows x 23 columns] 1print(net.line) name std_type from_bus to_bus length_km r_ohm_per_km \ 0 Line NAYY 4x50 SE 1 2 0.1 0.642 x_ohm_per_km c_nf_per_km g_us_per_km max_i_ka df parallel type \ 0 0.083 210.0 0.0 0.142 1.0 1 cs in_service 0 True 1print(net.load) name bus p_kw q_kvar const_z_percent const_i_percent sn_kva \ 0 Load 2 100.0 50.0 0.0 0.0 NaN scaling in_service type 0 1.0 True None 1print(net.ext_grid) name bus vm_pu va_degree in_service 0 Grid Connection 0 1.02 0.0 True 4.2 运行一个电力潮流可以采用runpp函数运行潮流计算 1pp.runpp(net) 当运行一个电力潮流计算，pandapower将所有的元件信息融合到一个pypower的case文件中，并使用pypower运行电力潮流。运行结果将在被处理完后写回到pandapower。 对于三节点示例网络，运行结果如下所示： 1print(net.res_bus) vm_pu va_degree p_kw q_kvar 0 1.020000 0.000000 -107.265391 -52.675195 1 1.008843 -0.760126 0.000000 0.000000 2 0.964431 0.115859 100.000000 50.000000 1print(net.res_trafo) p_hv_kw q_hv_kvar p_lv_kw q_lv_kvar pl_kw ql_kvar i_hv_ka \ 0 107.265391 52.675195 -105.392391 -50.696119 1.872999 1.979076 0.003382 i_lv_ka loading_percent 0 0.167325 29.289513 1print(net.res_line) p_from_kw q_from_kvar p_to_kw q_to_kvar pl_kw ql_kvar i_from_ka \ 0 105.392391 50.696119 -100.0 -50.0 5.392391 0.696119 0.167325 i_to_ka i_ka loading_percent 0 0.167326 0.167326 117.835208 1print(net.res_load) p_kw q_kvar 0 100.0 50.0 1print(net.res_ext_grid) p_kw q_kvar 0 -107.265391 -52.675195 所有其他的pandapower元件和网络分析函数（例如：最优潮流计算、状态评估和短路计算）也完全集成到表格形式的pandapower数据结构中。]]></content>
      <categories>
        <category>电力系统</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandapower</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandapower最优潮流计算tutorial]]></title>
    <url>%2F2018%2F12%2F01%2Fpandapower-opf-tutorial%2F</url>
    <content type="text"><![CDATA[pandapower最优潮流计算[pandapower Optimal Power Flow]This is an introduction into the usage of the pandapower optimal power flow. It shows how to set the constraints and the cost factors into the pandapower element tables. 该notebook主要是介绍pandapower最优潮流计算的使用。它展示了如何将约束和成本因子设置进入pandapower的元件列表。 示例网络[Example Network]We use the following four bus example network for this tutorial: 我们的教程使用下图的4节点示例网络： We first create this network in pandapower: 我们首先在pandapower中创建网络： 123456789101112131415161718192021222324252627import pandapower as ppimport numpy as npnet = pp.create_empty_network()#create busesbus1 = pp.create_bus(net, vn_kv=220.)bus2 = pp.create_bus(net, vn_kv=110.)bus3 = pp.create_bus(net, vn_kv=110.)bus4 = pp.create_bus(net, vn_kv=110.)#create 220/110 kV transformerpp.create_transformer(net, bus1, bus2, std_type="100 MVA 220/110 kV")#create 110 kV linespp.create_line(net, bus2, bus3, length_km=70., std_type='149-AL1/24-ST1A 110.0')pp.create_line(net, bus3, bus4, length_km=50., std_type='149-AL1/24-ST1A 110.0')pp.create_line(net, bus4, bus2, length_km=40., std_type='149-AL1/24-ST1A 110.0')#create loadspp.create_load(net, bus2, p_kw=60e3, controllable = False)pp.create_load(net, bus3, p_kw=70e3, controllable = False)pp.create_load(net, bus4, p_kw=10e3, controllable = False)#create generatorseg = pp.create_ext_grid(net, bus1, min_p_kw = -1e9, max_p_kw = 1e9)g0 = pp.create_gen(net, bus3, p_kw=-80*1e3, min_p_kw=-80e3, max_p_kw=0,vm_pu=1.01, controllable=True)g1 = pp.create_gen(net, bus4, p_kw=-100*1e3, min_p_kw=-100e3, max_p_kw=0, vm_pu=1.01, controllable=True) 网损最小化[Loss Minimization]We specify the same costs for the power at the external grid and all generators to minimize the overall power feed in. This equals an overall loss minimization: 我们特别指出外部电网和所有的发电机具有相同的电能成本，并最小化总的电能供应。这相当于整体网损的最小化。 123costeg = pp.create_polynomial_cost(net, 0, 'ext_grid', np.array([-1, 0]))costgen1 = pp.create_polynomial_cost(net, 0, 'gen', np.array([-1, 0]))costgen2 = pp.create_polynomial_cost(net, 1, 'gen', np.array([-1, 0])) 1print(costeg, costgen1, costgen2) 0 1 2 pp.create_polynomial_cost()为元件的多项式成本创建一个条目，返回已经创建的成本条目的ID We run an OPF: 我们运行一个最优潮流计算： 1pp.runopp(net, verbose=True) The OPF cost definition has changed! Please check out the tutorial &apos;opf_changes-may18.ipynb&apos; or the documentation! PYPOWER Version 5.1.4, 27-June-2018 -- AC Optimal Power Flow Python Interior Point Solver - PIPS, Version 1.0, 07-Feb-2011 Converged! let’s check the results: 让我们看下结果： 1print(net.res_ext_grid) p_kw q_kvar 0 -56530.125753 -1974.470643 1print(net.res_gen) p_kw q_kvar va_degree vm_pu 0 -71313.391997 1969.654771 -3.712810 1.000009 1 -12299.771985 1451.159898 -3.712777 1.000010 Since all costs were specified the same, the OPF minimizes overall power generation, which is equal to a loss minimization in the network. The loads at buses 3 and 4 are supplied by generators at the same bus, the load at Bus 2 is provided by a combination of the other generators so that the power transmission leads to minimal losses. 因为所有的成本都看成相同的，最优潮流计算最小化总的发电量后的结果和网络的损失最小化结果相同。母线3和母线4的负荷由各自母线上的发电机供应，而母线2上的负荷是由其他发电机联合供应因此电能的传输达到了最小的损失。 个体发电机成本[Individual Generator Costs]Let’s now assign individual costs to each generator. 现在给每台发电机赋予各自的发电成本。 We assign a cost of 10 ct/kW for the external grid, 15 ct/kw for the generator g0 and 12 ct/kw for generator g1: 我们给外部电网赋予10ct/kW的成本，发电机g0赋予15ct/kW的成本以及发电机g1赋予12ct/kW的成本： 123net.polynomial_cost.c.at[costeg] = np.array([[-0.1, 0]])net.polynomial_cost.c.at[costgen1] = np.array([[-0.15, 0]])net.polynomial_cost.c.at[costgen2] = np.array([[-0.12, 0]]) And now run an OPF: 现在运行最优潮流运算: 1pp.runopp(net, verbose=True) The OPF cost definition has changed! Please check out the tutorial &apos;opf_changes-may18.ipynb&apos; or the documentation! PYPOWER Version 5.1.4, 27-June-2018 -- AC Optimal Power Flow Python Interior Point Solver - PIPS, Version 1.0, 07-Feb-2011 Converged! 1print(net.res_ext_grid) p_kw q_kvar 0 -144559.444871 -9193.066456 1print(net.res_gen) p_kw q_kvar va_degree vm_pu 0 -0.011910 -8601.802715 -16.426869 0.967619 1 -0.033788 -10594.678350 -13.481042 0.989756 We can see that all active power is provided by the external grid. This makes sense, because the external grid has the lowest cost of all generators and we did not define any constraints. 我们可以看出所有的有功都是由外部电网提供。这是合理的，因为外部电网拥有最低的发电成本，而且我们没有定义任何的约束。 The dispatch costs are given in net.res_cost: dispatch成本可以由net.res_cost给出： 1print(net.res_cost) 14455.950328285478 变压器约束[Transformer Constraint]Since all active power comes from the external grid and subsequently flows through the transformer, the transformer is overloaded with a loading of about 145%: 因为所有的有功功率都来自与外部电网，接着潮流流过了变压器，变压器过载运行，负载约为145%： 1print(net.res_trafo) p_hv_kw q_hv_kvar p_lv_kw q_lv_kvar pl_kw \ 0 144559.444871 9193.066457 -143959.516332 15993.742384 599.928539 ql_kvar i_hv_ka i_lv_ka loading_percent 0 25186.808841 0.380136 0.759988 144.85146 We now limit the transformer loading to 50%: 现在设置变压器的负载约束为50%： 1net.trafo["max_loading_percent"] = 50 (the max_loading_percent parameter can also be specified directly when creating the transformer)and run the OPF: （max_loading_percent参数也可以在创建变压器时直接指定）然后运行最优潮流计算： 1pp.runopp(net) The OPF cost definition has changed! Please check out the tutorial &apos;opf_changes-may18.ipynb&apos; or the documentation! We can see that the transformer complies with the maximum loading: 我们可以看到变压器以最大负荷运行： 1print(net.res_trafo) p_hv_kw q_hv_kvar p_lv_kw q_lv_kvar pl_kw \ 0 49953.864488 -2147.313884 -49833.813812 5167.453489 120.050676 ql_kvar i_hv_ka i_lv_ka loading_percent 0 3020.139605 0.131216 0.262153 49.999995 And power generation is now split between the external grid and generator 1 (which is the second cheapest generation unit): 现在有功功率的产生主要来自于外部电网和发电机g1（g1是发电成本第二低的发电机组） 1print(net.res_ext_grid) p_kw q_kvar 0 -49953.864488 2147.313884 1print(net.res_gen) p_kw q_kvar va_degree vm_pu 0 -0.005754 -2993.145845 -6.232829 0.985230 1 -93304.077572 -3453.343461 -1.237884 1.025709 This comes of course with an increase in dispatch costs: 这样的结果就导致了dispatch成本的上升： 1print(net.res_cost) 16191.876620556373 线路负载约束[Line Loading Constraints]We now look at the line loadings: 我们现在关注于线路负载： 1print(net.res_line) p_from_kw q_from_kvar p_to_kw q_to_kvar pl_kw \ 0 19780.009515 -2479.435419 -19341.695290 1104.393239 438.314225 1 -50658.298957 1888.752606 52783.697109 921.064101 2125.398152 2 30520.380464 2532.279360 -29946.195703 -2688.018070 574.184761 ql_kvar i_from_ka i_to_ka i_ka loading_percent 0 -1375.042180 0.104309 0.103207 0.104309 22.193320 1 2809.816707 0.270061 0.270140 0.270140 57.476548 2 -155.738710 0.156712 0.157323 0.157323 33.472994 and run the OPF with a 50% loading constraint: 然后在线路50%负载水平条件下运行最优潮流计算： 12net.line["max_loading_percent"] = 50pp.runopp(net, verbose=True) The OPF cost definition has changed! Please check out the tutorial &apos;opf_changes-may18.ipynb&apos; or the documentation! PYPOWER Version 5.1.4, 27-June-2018 -- AC Optimal Power Flow Python Interior Point Solver - PIPS, Version 1.0, 07-Feb-2011 Converged! Now the line loading constraint is complied with: 现在线路的负载约束编译后： 1print(net.res_line) p_from_kw q_from_kvar p_to_kw q_to_kvar pl_kw \ 0 16727.519781 -3194.974086 -16412.725022 1534.133994 314.794759 1 -44451.195382 868.876229 46059.937188 830.815836 1608.741806 2 27533.080431 4051.080417 -27060.140775 -4429.078165 472.939657 ql_kvar i_from_ka i_to_ka i_ka loading_percent 0 -1660.840092 0.088849 0.087130 0.088849 18.903945 1 1699.692065 0.234999 0.235000 0.235000 49.999998 2 -377.997748 0.141964 0.143057 0.143057 30.437636 And all generators are involved in supplying the loads: 所有的发电机都开始供应负荷： 1print(net.res_ext_grid) p_kw q_kvar 0 -49787.59354 4603.760721 1print(net.res_gen) p_kw q_kvar va_degree vm_pu 0 -9136.079301 -2403.009062 -5.814954 0.992994 1 -83593.017290 -4881.895570 -1.511656 1.028899 This of course comes with a once again rising dispatch cost: 这导致了dispatch成本又一次的提升： 1print(net.res_cost) 16380.333324014753 电压约束[Voltage Constraints]Finally, we have a look at the bus voltage: 最后，我们在关注一下电压： 1print(net.res_bus) vm_pu va_degree p_kw q_kvar lam_p lam_q 0 1.000000 0.000000 -49787.593540 4603.760721 100.000000 -2.642153e-21 1 1.006025 -3.408832 60000.000000 0.000000 130.952263 -5.410268e-01 2 0.992994 -5.814954 60863.920699 -2403.009062 149.999973 1.192009e-21 3 1.028899 -1.511656 -73593.017290 -4881.895570 120.000014 2.988530e-21 and constrain it: 加以约束，并运行最优潮流计算： 123net.bus["min_vm_pu"] = 1.0net.bus["max_vm_pu"] = 1.02pp.runopp(net) The OPF cost definition has changed! Please check out the tutorial &apos;opf_changes-may18.ipynb&apos; or the documentation! We can see that all voltages are within the voltage band: 1print(net.res_bus) vm_pu va_degree p_kw q_kvar lam_p lam_q 0 1.000000 0.000000 -49906.847219 3050.617054 100.000000 -6.408104e-22 1 1.004168 -3.421015 60000.000000 0.000000 131.268596 -2.133663e-01 2 1.000000 -5.976094 59278.204655 -14858.933373 149.999992 3.120534e-21 3 1.020000 -1.366892 -71863.491321 9172.691847 120.000005 -1.926105e-21 And all generators are once again involved in supplying the loads: 所有的发电机又一次都参与了负荷的供应： 1print(net.res_ext_grid) p_kw q_kvar 0 -49906.847219 3050.617054 1print(net.res_gen) p_kw q_kvar va_degree vm_pu 0 -10721.795345 -14858.933373 -5.976094 1.00 1 -81863.491321 9172.691847 -1.366892 1.02 This of course comes once again with rising dispatch costs: 又一次导致了dipatch成本的提升： 1print(net.res_cost) 16422.572982172846 直流最优潮流计算[DC OPF]pandapower also provides the possibility of running a DC Optimal Power Flow: pandapower也可以提供了运行直流最优潮流计算的可能： 1pp.rundcopp(net) Since voltage magnitudes are not included in the DC power flow formulation, voltage constraints canot be considered in the DC OPF: 因为直流潮流公式中不考虑电压幅值，所以在直流潮流计算中并不考虑电压约束： 1print(net.res_bus) vm_pu va_degree p_kw q_kvar lam_p lam_q 0 NaN 0.000000 -49999.999965 NaN 100.000000 0.0 1 NaN -3.436967 60000.000000 NaN 130.909091 0.0 2 NaN -5.708566 61488.746680 NaN 150.000000 0.0 3 NaN -1.362340 -71488.746715 NaN 120.000000 0.0 Line and transformer loading limits are however complied with: 线路和变压器负载限制编译后为： 1print(net.res_line) p_from_kw q_from_kvar p_to_kw q_to_kvar pl_kw ql_kvar \ 0 16715.233329 0.0 -16715.233329 0.0 0.0 0.0 1 -44773.513351 0.0 44773.513351 0.0 0.0 0.0 2 26715.233363 0.0 -26715.233363 0.0 0.0 0.0 i_from_ka i_to_ka i_ka loading_percent 0 0.087732 0.087732 0.087732 18.666430 1 0.235000 0.235000 0.235000 50.000000 2 0.140219 0.140219 0.140219 29.833747 1print(net.res_trafo) p_hv_kw q_hv_kvar p_lv_kw q_lv_kvar pl_kw ql_kvar i_hv_ka \ 0 49999.999965 0.0 -49999.999965 0.0 0.0 0.0 0.131216 i_lv_ka loading_percent 0 0.262432 50.0 As are generator limits: 发电机限制为： 1print(net.gen) name bus p_kw vm_pu sn_kva min_q_kvar max_q_kvar scaling \ 0 None 2 -80000.0 1.01 NaN NaN NaN 1.0 1 None 3 -100000.0 1.01 NaN NaN NaN 1.0 in_service type min_p_kw max_p_kw controllable 0 True None -80000.0 0.0 True 1 True None -100000.0 0.0 True 1print(net.res_gen) p_kw q_kvar va_degree vm_pu 0 -8511.253320 NaN -5.708566 1.0 1 -81488.746715 NaN -1.362340 1.0 The cost function is the same for the linearized OPF as for the non-linear one: 线性最优潮流计算和非线性潮流计算的成本函数一致： 1print(net.res_cost) 16055.337600298368 Piecewise linear cost functionsThe OPF also offers us piecewise linear cost functions. Let us first check the actual cost function setup: 分段线性成本函数最优潮流九三也提供了分段线性成本函数。我们首先看一下实际的成本函数设置： 1print(net.polynomial_cost) type element element_type c 0 p 0 ext_grid [[-0.1, 0.0]] 1 p 0 gen [[-0.15, 0.0]] 2 p 1 gen [[-0.12, 0.0]] An element can either have polynomial costs or piecewise linear costs at the same time. So let us first delete the polynomial costs in order to avoid confusion and errors: 一个元件的成本函数只能是多项式或者分段线性中的一种。因此，让我们首先删除多项式成本函数，以免产生困惑和错误： 1net.polynomial_cost= net.polynomial_cost.drop(net.polynomial_cost.index.values) The results above have been produced with polynomial cost functions, that were linear. Let’s try to reproduce the results using piecewise linear cost functions. Note: The cost functions need to have the same gradient! 上面的计算结果是采用多项式成本函数计算的，是线性的。现在我们使用分段线性函数重新计算结果。需要强调的是：成本函数需要拥有相同的梯度！ 123pp.create_piecewise_linear_cost(net, 0, "gen", np.array([[-1 , 0.15] ,[0, 0]]))pp.create_piecewise_linear_cost(net, 1, "gen", np.array([[-1, 0.12], [0, 0]]))pp.create_piecewise_linear_cost(net, 0, "ext_grid", np.array([[-1, 0.1], [0, 0]])) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-42-5d2a2efd8ee4&gt; in &lt;module&gt; ----&gt; 1 pp.create_piecewise_linear_cost(net, 0, &quot;gen&quot;, np.array([[-1 , 0.15] ,[0, 0]])) 2 pp.create_piecewise_linear_cost(net, 1, &quot;gen&quot;, np.array([[-1, 0.12], [0, 0]])) 3 pp.create_piecewise_linear_cost(net, 0, &quot;ext_grid&quot;, np.array([[-1, 0.1], [0, 0]])) ~/.local/lib/python3.5/site-packages/pandapower/create.py in create_piecewise_linear_cost(net, element, element_type, data_points, type, index) 2469 if not (net[element_type].max_p_kw.at[element] &lt;= max(p) and 2470 net[element_type].min_p_kw.at[element] &gt;= min(p)): -&gt; 2471 raise ValueError(&quot;Cost function must be defined for whole power range of the &quot; 2472 &quot;generator&quot;) 2473 if type == &quot;q&quot;: ValueError: Cost function must be defined for whole power range of the generator What we forgot is that the piecewise linear function should be defined for the whole range of the generator. The range is determined by p_max and p_min. Let’s check: 额…错误信息是ValueError: Cost function must be defined for whole power range of the generator，让我们检查一下： 1print(net.gen.max_p_kw) 0 0.0 1 0.0 Name: max_p_kw, dtype: float64 1print(net.gen.min_p_kw) 0 -80000.0 1 -100000.0 Name: min_p_kw, dtype: float64 We try again: 我们再试一下： 12pp.create_piecewise_linear_cost(net, 0, "gen", np.array([[-80000* 1 , 80000*0.15], [0, 0]]))pp.create_piecewise_linear_cost(net, 1, "gen", np.array([[-100000*1, 100000*0.12], [0, 0]])) 1 An external grid usually has no operational limits, but this is a problem for the OPF: 外部电网通常没有运行限制，但这是OPF的一个问题： 1pp.create_piecewise_linear_cost(net, 0, "ext_grid", np.array([[0, 0], [1, 0.1]])) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-46-e06a56395857&gt; in &lt;module&gt; ----&gt; 1 pp.create_piecewise_linear_cost(net, 0, &quot;ext_grid&quot;, np.array([[0, 0], [1, 0.1]])) ~/.local/lib/python3.5/site-packages/pandapower/create.py in create_piecewise_linear_cost(net, element, element_type, data_points, type, index) 2469 if not (net[element_type].max_p_kw.at[element] &lt;= max(p) and 2470 net[element_type].min_p_kw.at[element] &gt;= min(p)): -&gt; 2471 raise ValueError(&quot;Cost function must be defined for whole power range of the &quot; 2472 &quot;generator&quot;) 2473 if type == &quot;q&quot;: ValueError: Cost function must be defined for whole power range of the generator So we set imaginary constraints, that we can choose very broad: 因此，我们虚构一个约束，将这个约束的范围设置的很广： 12net.ext_grid["max_p_kw"] = 1e9net.ext_grid["min_p_kw"] = -1e9 1print(net.ext_grid) name bus vm_pu va_degree in_service min_p_kw max_p_kw 0 None 0 1.0 0.0 True -1.000000e+09 1.000000e+09 1pp.create_piecewise_linear_cost(net, 0, "ext_grid", np.array([[-1e9, 1e9*.1], [1e9, -1e9*0.1]])) 2 Let us check the results from the previous OPF again! 让我们再看一下之前的OPF运行结果： 1print(net.res_bus) vm_pu va_degree p_kw q_kvar lam_p lam_q 0 NaN 0.000000 -49999.999965 NaN 100.000000 0.0 1 NaN -3.436967 60000.000000 NaN 130.909091 0.0 2 NaN -5.708566 61488.746680 NaN 150.000000 0.0 3 NaN -1.362340 -71488.746715 NaN 120.000000 0.0 1print(net.res_cost) 16055.337600298368 We run the same OPF now with different cost function setup. We should get the exact same results: 我们运行了相同的OPF但是采用不同的成本函数，我们应该获得完全相同的结果： 1pp.rundcopp(net) 1print(net.res_cost) 16055.337600298368 1print(net.res_bus) vm_pu va_degree p_kw q_kvar lam_p lam_q 0 NaN 0.000000 -49999.999965 NaN 100.000000 0.0 1 NaN -3.436967 60000.000000 NaN 130.909091 0.0 2 NaN -5.708566 61488.746680 NaN 150.000000 0.0 3 NaN -1.362340 -71488.746715 NaN 120.000000 0.0]]></content>
      <categories>
        <category>电力系统</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandapower</tag>
        <tag>电力系统</tag>
        <tag>潮流计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04在apt-get时遇到initramfs-tools错误]]></title>
    <url>%2F2018%2F11%2F22%2FUbuntu16-04%E5%9C%A8apt-get%E6%97%B6%E9%81%87%E5%88%B0initramfs-tools%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[Ubuntu16.04在apt-get时遇到initramfs-tools错误，错误描述如下： 1234567891011121314151617181920212223yanbin@laptop:~$ sudo apt-get install -f[sudo] password for yanbin: Reading package lists... DoneBuilding dependency tree Reading state information... Done0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.1 not fully installed or removed.After this operation, 0 B of additional disk space will be used.Setting up initramfs-tools (0.122ubuntu8.13) ...update-initramfs: deferring update (trigger activated)Processing triggers for initramfs-tools (0.122ubuntu8.13) ...update-initramfs: Generating /boot/initrd.img-4.15.0-39-genericW: Possible missing firmware /lib/firmware/i915/kbl_guc_ver9_14.bin for module i915W: Possible missing firmware /lib/firmware/i915/bxt_guc_ver8_7.bin for module i915gzip: stdout: No space left on deviceE: mkinitramfs failure cpio 141 gzip 1update-initramfs: failed for /boot/initrd.img-4.15.0-39-generic with 1.dpkg: error processing package initramfs-tools (--configure): subprocess installed post-installation script returned error exit status 1Errors were encountered while processing: initramfs-toolsE: Sub-process /usr/bin/dpkg returned an error code (1) 解决步骤： uname -r 查询当前的内核版本号 我的内核版本号为4.15.0-39-generic cd进入/boot目录下，输入命令dpkg –get-selections | grep linux 12345678910111213141516171819202122232425yanbin@laptop:/boot$ dpkg --get-selections | grep linuxconsole-setup-linux installlibselinux1:amd64 installlinux-base installlinux-firmware installlinux-generic-hwe-16.04 installlinux-headers-4.15.0-29 installlinux-headers-4.15.0-29-generic installlinux-headers-4.15.0-39 installlinux-headers-4.15.0-39-generic installlinux-headers-generic-hwe-16.04 installlinux-image-4.15.0-29-generic installlinux-image-4.15.0-39-generic installlinux-image-generic-hwe-16.04 installlinux-libc-dev:amd64 installlinux-modules-4.15.0-29-generic installlinux-modules-4.15.0-39-generic installlinux-modules-extra-4.15.0-29-generic installlinux-modules-extra-4.15.0-39-generic installlinux-sound-base installpptp-linux installsyslinux installsyslinux-common installsyslinux-legacy installutil-linux install 删除所有旧版本, sudo apt-get purge linux-modules-4.15.0-29-generic，比如我删除所有的29版本 删除之后，输入命令ls -l，结果如下 12345678910111213yanbin@laptop:/boot$ ls -ltotal 67380-rw-r--r-- 1 root root 1537997 10月 25 04:07 abi-4.15.0-39-generic-rw-r--r-- 1 root root 217026 10月 25 04:07 config-4.15.0-39-genericdrwxr-xr-x 5 root root 1024 11月 21 20:52 grub-rw-r--r-- 1 root root 54504002 11月 21 20:52 initrd.img-4.15.0-39-genericdrwx------ 2 root root 12288 11月 21 2018 lost+found-rw-r--r-- 1 root root 182704 1月 28 2016 memtest86+.bin-rw-r--r-- 1 root root 184380 1月 28 2016 memtest86+.elf-rw-r--r-- 1 root root 184840 1月 28 2016 memtest86+_multiboot.bin-rw-r--r-- 1 root root 0 10月 25 04:07 retpoline-4.15.0-39-generic-rw------- 1 root root 4046310 10月 25 04:07 System.map-4.15.0-39-generic-rw------- 1 root root 8121560 10月 25 18:43 vmlinuz-4.15.0-39-generic]]></content>
      <categories>
        <category>Ubuntu基础</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04搭建python3环境]]></title>
    <url>%2F2018%2F09%2F23%2FUbuntu%E6%90%AD%E5%BB%BApython3%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[Ubuntu16.04系统内置了Python2和Python3，默认情况下是Python2。 我个人常用Python3，所以首先将Python3设置为默认。 12sudo update-alternatives --install /usr/bin/python python /usr/bin/python2 100sudo update-alternatives --install /usr/bin/python python /usr/bin/python3 150 测试一下是否设置成功，如果显示python 3.5.2表示设置成功。 1python -V 系统自带的python是没有内置pip工具的，想要安装其他库，首先安装一下python的pip工具。 1sudo apt-get install python3-pip 安装后就可以使用 pip3 install package 安装库了。]]></content>
      <categories>
        <category>Ubuntu基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GBDT和XGBoost整理]]></title>
    <url>%2F2018%2F09%2F18%2FGBDT%E5%92%8CXGBoost%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[百度“一面之缘”，还是非常感谢百度机器学习的面试官，给了很多中肯的建议，谢谢！ 本篇文章主要梳理一下GBDT和XGBoost之间的关系以及各自的知识点。 DTQuestionsQ1：决策树可以做什么？决策树分为分类树和回归树。 分类树 分类决策树是一种有监督学习，即给定数据集中的每个样本都含有一组属性（即特征）和一个标签（即类别）。 分类树通常采用信息增益或信息增益比来划分节点，每个节点样本的类别情况投票决定测试样本的类别。 决策树可以表示给定特征条件下的条件概率分布。决策树对特征空间进行划分，得到互不相交的区域，并在每个区域上定义一个类的概率分布，构成了一个条件概率分布。 假设X表示特定划分后特征空间中的区域集合，Y表示类的集合。 二分类中，当$P(Y=+1|X=r_i)\geq0.5$，即将该叶子节点归为正类；否则归为负类。 回归树 常见的回归决策树有CART。CART既可以做分类，也可以做回归。 回归树使用最小化均方误差划分节点，每个节点的样本均值作为预测样本的回归预测值。 回归树采用启发式方法，首先遍历特征空间中的所有特征，找到最优的切分特征和该特征的最优分割点，即$x_j$和$s$，将特征空间划分为两部分，并重复这个操作。 与分类决策树不同的是，分类决策树的是通过信息增益来划分特征空间，而回归决策树是通过划分后两部分分别的误差平方和的和最小来划分的，即$\sum_{x_i \in R_m} (y_i - f(x_i))$，其中$f(x_i)$是该划分区域的预测值，即每个区域样本标签的平均值，$f(x_i) = c_m = ave(y_i|x_i\in R_m)$ GBDTBoosting的优秀代表，对函数残差近似值进行梯度下降。 GBDT的核心在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后得到真实值的累加量。 GBDT的损失函数这里我常常和CART的损失函数搞混。CART节点分裂时的损失函数是根据分裂前后的信息增益，通常采用信息熵或者Gini系数。而GBDT模型可以分为分类模型和回归模型，两者的损失函数是不一样的。 分类模型 GradientBoostingClassifier deviance[对数似然损失函数][默认]，类似与逻辑回归，可以做二分类和多分类，计算条件概率，输出概率 exponential[指数损失函数]，此时GradientBoostingClassifier退化到AdaBoost 回归模型 GradientBoostingRegressor ls[最小均方差][默认]，适用于数据噪音少的情况 lad[最小绝对误差] huber，适用于数据噪音多的情况。 quantile，适用于对数据集进行分段预测的时候 QuestionsQ1: 怎样设置单颗树的停止条件？ 节点分裂时的最小样本数 min_samples_split 叶子节点的最小样本数 min_samples_leaf 最大深度 max_depth 最大叶子节点数 max_leaf_nodes 最小不纯度衰减量 min_impurity_decrease XGBoostXGBoost是Boosting集大成者，对函数残差近似值进行梯度下降，迭代时利用了二阶梯度。 以下是陈天奇博士论文XGBoost: A Scalable Tree Boosting System的原文翻译。 正则化的学习目标函数(Regularized Learning Objective)在给定n个样本m个特征的数据集，一个集成模型使用了K个基模型线性组合后预测输出。 $$\widehat{y}_i = \phi(x_i) = \sum ^K _{k=1} f_k(x_i), f_k \in F$$ 其中：$F={f(x)=w_{q(x)}}(q:R^m \rightarrow T, w \in R^T)$表示回归树空间（通常值得是CART）。q表示样本到相应树上叶子节点索引的映射。T是回归树叶子节点数目。每一个$f_k$对应着一个独立树结构q和叶子节点权重w。与决策树不同的是（这里我想应该是指代分类决策树），每一颗回归树的叶子节点上包含了一个连续的socre，我们使用$w_i$表示第i个节点的score。 给出正则化目标函数$L(\phi) = \sum_il(\widehat{y}_i, y_i) + \sum_k \Omega(f_k)$ 其中：$\Omega(f) = \gamma T + \frac{1}{2}\lambda |w|^2$，$l$可以是不同的凸损失函数，对预测值$\widehat{y}$和目标值$y$的不同程度进行度量。第二项$\Omega$惩罚了模型的复杂度。额外的正则化项有助于平滑最终的学习权重避免过拟合。正则化目标函数趋向于选择一个简单且有预测能力的模型。我们的目标函数和相应的学习算法更容易并行处理。当正则化参数设置为0时，目标函数回退到传统的梯度提升树。 梯度提升树(Gradient Tree Boosting)$\widehat{y}_i^{(t)}$表示第$t$次第$i$个实例，我们需要添加$f_t$最小化目标函数。 $$L^{(t)}=\sum_{i=1}^nl(y_i, \widehat{y}_i)+\Omega(f_t)$$ $$L^{(t)} = \sum_{i=1}^nl(y_i,\widehat{y}_i^{(t-1)} + f_t(x_i)) + \Omega(f_t)$$ 我们贪婪地添加最能提升模型性能参数的$f_t$，二阶泰勒展开近似可以快速优化我们的目标。 二阶泰勒展开： $$L^{(t)} \simeq \sum_{i=1}^n[l(y_i, \widehat{y}_i^{(t-1)}) +g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i)] + \Omega(f_t)$$ 其中，$g_i = \partial_{\widehat{y}_i^{(t-1)}} l(y_i, \widehat{y}_i^{(t-1)})$，$h_i = \partial_{\widehat{y}_i^{(t-1)}}^2 l(y_i, \widehat{y}_i^{(t-1)})$分别为损失函数一阶和二阶的梯度统计参数，然后我们移除常数项可以得到 $$ \widetilde{L}^{(t)} = \sum_{i=1}^n [g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i)] + \Omega(f_t) $$ 接着，定义$I_j = {i|q(i)=j}$表示叶子节点j上的样本集合，改写上式可以得到 $$ \widetilde{L}^{(t)} = \sum_{i=1}^n [g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i)] + \gamma T + \frac{1}{2}\lambda\sum_{j=1}^Tw_j^2$$ $$ \widetilde{L}^{(t)} = \sum_{j=1}^T [(\sum_{i\in j}g_i)w_j + \frac{1}{2}(\sum_{i\in j}h_i+\lambda)w_j^2] + \gamma T $$ 对于一个固定的树结构$q(x)$，我们可以计算出叶子节点j最优的权重$w_j^*$，即 $$ w_j^* = -\frac{\sum_{i\in j}g_i}{\sum_{i\in j}h_i+\lambda} $$ 从而计算出目标函数相应的最优值 $$ \widetilde{L}^{(t)} = -\frac{1}{2} \sum_{j=1}^T \frac{\sum_{i\in j}g_i}{\sum_{i\in j}h_i+\lambda} + \gamma T $$ 上式可以作为一个度量一个树结构好坏的得分函数(scoring function)，该得分函数类似于评价决策树的不纯度得分，不同的是他可以使用多种多样的目标函数。 正常来说，遍历所有的树结构q是不可能的。所以通常使用贪婪算法，从一个叶子节点开始，迭代地添加支路。假设$I_L$和$I_R$是分裂后左节点和右节点的样本集合，令$I=I_L \cup I_R$，那么给出分裂后损失函数减少量的计算公式 $$ L_{split} = \frac{1}{2} [\frac{\sum_{i\in I_L}g_i}{\sum_{i\in I_L}h_i+\lambda} + \frac{\sum_{i\in I_R}g_i}{\sum_{i\in I_R}h_i+\lambda} - \frac{\sum_{i\in I}g_i}{\sum_{i\in I}h_i+\lambda}] + \gamma $$ 上式通常用来评价分裂候选节点，即选出需要分裂的节点 分裂搜算法精确贪婪算法(Exact Greedy Algorithm) 近似算法 贪婪算法很强大，因为它贪婪地遍历了所有可能的分割points。但是，当数据集无法完全加载到内存中的，算法的效率很低。在处理分布式数据的时候同样会遇到这样的问题。因此，在这两种情况中，为了梯度提升树算法的效率，近似算法是必要的。 QuestionsQ1：XGBoost如何并行？ 以下是陈天奇论文原文翻译。 在精确贪婪算法中，我们在一个block中存储整个数据集，然后运行分割搜索算法，该算法是通过线性的扫描预先排序好的条目。我们统一对所有的叶子节点进行分割搜索，这样一次扫描我们就可以搜集叶子支路上所有分割候选者的统计信息。 当使用近似算法的时候，block结构也能起作用。在这种情况下，将使用多个blocks。每一个block都对应着数据集的行子集。不同的block可以通过多个机器进行分布式，也可以存储在disk in the out-of-core setting。 XGBoost并行不是在Tree粒度上并行的，而是在特征粒度上并行的。 Q2：给你一堆数据，请解释数据是如何在XGBoost模型内部运行的？ 使用 通用参数 进行宏观上的设置。 booster 参数选择树学习器还是线性学习器。 silent 设置训练时是否输出训练的内容，比如树的深度、叶子节点的个数。 nthread 设置训练时的进程数。 设置学习的目标参数。 objective 设置模型的功能，是分类or回归，是二分类还是多分类，例如二分类时，可以采用 rank: pairwise。然后设置目标函数， eval_metric设置评估函数（也可以成为损失函数），比如auc。XGBoost的目标函数包含损失函数+正则项。 定义正则化参数，正则化参数控制了模型的复杂程度。主要包含三个参数： gamma——控制树的节点数量，该参数越大，算法越保守。 lambda——控制叶子节点score，该参数越大，算法越保守。 确定目标函数后，进入训练过程。 基于数据集，拟合一颗CART决策树，训练完一个结果后，一棵树的结果一定是不准确的。经过评估函数评估误差后，进入下一颗树的生成过程。 第二颗树不是预测原目标，而是目标与上一颗树的残差，以目标函数的负梯度作为第二颗树的学习目标。 第三棵树学习前两棵树的总残差。这样学习，理论上我们学习的误差在一步步的减少。 …… 反复迭代，知道迭代次数等于 n_estimators 将之前所有的树加起来作为最后的训练结果。我们可以看到，将每棵树的值加起来，实际上是从原目标函数中每次都加上一个负梯度，也就相当于减去一个负梯度。这也是GBDT梯度下降的原理及核心。 以上参考文章LightGBM，XGBoost被面試官刁難了？內有含淚面試經驗 Q3：XGBoost中的 objectiveobjective参数用来设置模型的任务即目的。可选的任务类型有： reg:linear: 线性回归 reg:logistic: 逻辑回归 binary:logistic: 适用于二分类的逻辑回归，输出概率 binary:logitraw: 适用于二分类的逻辑回归，输出logistic transformation之前的score，即$w^Tx$ binary:hinge: 适用于二分类的hinge loss。该设置是的预测值为0或者1，而不是产生概率。 gpu:reg:linear, gpu:reg:logistic, gpu:binary:logistic, gpu:binary:logitraw: 相应目标函数的GPU版本。需要注意的是，类似与GPU直方图算法，只有当整个训练部分使用相同的数据集时才可以使用。 count:poisson –poisson regression for count data, output mean of poisson distribution max_delta_step is set to 0.7 by default in poisson regression (used to safeguard optimization) survival:cox: Cox regression for right censored survival time data (negative values are considered right censored). Note that predictions are returned on the hazard ratio scale (i.e., as HR = exp(marginal_prediction) in the proportional hazard function h(t) = h0(t) * HR). multi:softmax: XGBoost使用softmax函数处理多分类问题，同时需要设置 num_class ，即类别的个数 multi:softprob: 与softmax一样，但同时输出ndata nclass的向量，同时可以将该向量转化为ndata nclass的矩阵。每行表示样本属于每个类别的概率。 rank:pairwise: 使用LambdaMART进行pairwise排序，最小化pairwise损失函数。 rank:ndcg: Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized rank:map: Use LambdaMART to perform list-wise ranking where Mean Average Precision (MAP) is maximized reg:gamma: gamma regression with log-link. Output is a mean of gamma distribution. It might be useful, e.g., for modeling insurance claims severity, or for any outcome that might be gamma-distributed. reg:tweedie: Tweedie regression with log-link. It might be useful, e.g., for modeling total loss in insurance, or for any outcome that might be Tweedie-distributed. Q4：为什么XGBoost要用泰勒展开，优势在哪里？ 题目来自七月在线面试题库 xgboost使用了一阶和二阶偏导，利用了泰勒公式的二街展开，优点如下： 二阶导数使得梯度下降的更准更快 可以在不选定损失函数具体形式的情况下, 仅仅依靠输入数据的值就可以进行叶子分裂优化计算, 本质上也就把损失函数的选取和模型算法优化/参数选择分开了. 这种去耦合增加了xgboost的适用性, 使得它按需选取损失函数, 可以用于分类, 也可以用于回归。 Q5：xgboost如何寻找最优特征？是有放回还是无放回的呢？ 题目来自七月在线面试题库 xgboost如何寻找最优特征？ 每一轮训练的过程中，计算$L_{split}$，给出每个特征的最大增益，最大增益的特征作为分裂依据，从而记忆了每个特征在模型训练时的重要度信息——从根到叶子节点之间的节点中涉及到某特征的次数作为该特征的重要性排序。 是有放回还是无放回的呢？ xgboost属于boosting集成学习方法, 样本是不放回的, 因而每轮计算样本不重复. 另一方面, xgboost支持子采样, 也就是每轮计算可以不使用全部样本, 以减少过拟合. 进一步地, xgboost 还有列采样, 每轮计算按百分比随机采样一部分特征, 既提高计算速度又减少过拟合。 Q6：xgboost是如何处理缺失值的呢？ 答案来自怎么理解决策树、xgboost能处理缺失值？而有的模型(svm)对缺失值比较敏感呢? 根据论文作者Tianqi Chen在论文中介绍，xgboost把缺失值当做稀疏矩阵来处理，本身节点分裂时不考虑缺失值，缺失值会被放到左子树和右子树,然后分别计算损失，选择较优的那一个，如果训练时没有出现缺失值，预测时出现了缺失值，那么默认被分类到右子树。 GBDT和XGBoost之间的关系XGBoost比GBDT好在哪里？ XGBoost速度更快，因为它可以在特征粒度上进行并行计算 XGBoost利用泰勒展开式中的二阶导数 XGBoost加入了正则项，gamma用于控制树的节点个数，lambda用于控制权重w的L2模的平方和，降低了模型的方差，简化了模型，提高了模型的泛化能力 XGBoost支持列抽样col_sample，防止过拟合，还能j减少计算。 XGBoost支持对缺失值进行处理，对于有缺失值的样本，XGBoost可以自己学习出分裂它的方向。 XGBoost不仅支持CART基分类器，还支持线性分类器，如linear、lr XGBoost支持自定义损失函数，只要函数满足二阶可导 XGBoost支持对稀疏数据进行处理 参考文献[1] RF、GBDT、XGBoost常见面试题整理]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>XGBoost</tag>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[牛客网在线编程Python3输入输出问题]]></title>
    <url>%2F2018%2F08%2F04%2Fthe-Python3-input-and-output-in-nowcoder-online-program%2F</url>
    <content type="text"><![CDATA[使用Python3进行牛客网在线编程时发现了一个问题时间超限，代码如下: 1234567import syswhile True: try: line = sys.stdin.readline() except: break 自测结果如下： 自测结果 不通过 时间超限。（运行时间：2001ms，占用内存：0k） 查了官方的牛客网在线判题系统使用帮助，上面读取输入的方法应该适合于Python2，下面给出Python3读取输入的标准写法，其他语言的可以去原文查看。 Python3 1234import sys for line in sys.stdin: a = line.split() print(int(a[0]) + int(a[1]))]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost调参]]></title>
    <url>%2F2018%2F07%2F22%2FSet-XGBoost-Parameters%2F</url>
    <content type="text"><![CDATA[XGBClassifier class xgboost.XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective=’binary:logistic’, booster=’gbtree’, n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs) Bases: xgboost.sklearn.XGBModel, object XGBoost classification的scikit-learn API接口。 XGBClassifier.fit fit(X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None) 拟合gradient boosting classifier sklearn.model_selection.GridSearchCV class sklearn.model_selection.GridSearchCV(estimator, param_grid, scoring=None, fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch=‘2*n_jobs’, error_score=’raise’, return_train_score=’warn’) 调参步骤第一步：]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost参数设置]]></title>
    <url>%2F2018%2F07%2F21%2FXGBoost-Parameters%2F</url>
    <content type="text"><![CDATA[XGBoost Parameters 在运行XGBoost之前，我们必须设置三种类型的参数：general parameters, booster parameters and task parameters. General parameters 与我们使用哪一种提升器进行提升有关，通常可以选择树或者线性模型。 Booster parameters 取决于你选的提升器 Learning task parameters 取决于学习场景。例如：做回归任务时可能会使用不同的参数进行排名任务。 Command line 与CLI版本的XGBoost有关。 NoteR模块中的参数在R模块中可以使用.(点)在参数替换_(下划线)，例如：你可以使用max.depth 代替max_depth. 带有下划线的参数在R中同样是可用的。 General parameters booster [default= gbtree ] 决定使用哪种提升器(booster)。可以是gbtree, gblinear or dart; gbtree和dart使用树模型而gblinear使用线性函数. silent [default=0] 0表示打印运行运行信息；1表示静默模式。 nthread [default to maximum number of threads available if not set] 运行XGBoost的并行线程数 num_pbuffer [set automatically by XGBoost, no need to be set by user][XGBoost自动设置，用户不必设置] 预测缓冲区的大小。通常设置为训练示例数量。缓冲区用来保存提升过程最后一步的预测结果。 num_feature [set automatically by XGBoost, no need to be set by user][XGBoost自动设置，用户不必设置] 提升过程中使用的特征尺度，设置为特征的最大尺度。 Booster parametersParameters for Tree Booster eta [default=0.3, alias: learning_rate][默认0.3，又称学习率] 在更新权重过程中使用步长缩小以防止过拟合。在每一步提升之后，我们可以直接获得新特征的权重，eta缩小了特征权重的变化，使得提升过程更加保守。 大小区间: [0,1] gamma [default=0, alias: min_split_loss][默认0，又称最小分割损失] 在树的叶节点上进行进一步分割所需要的最小损失减少量。gamma越大，算法就越保守。 大小区间: [0,∞] max_depth [default=6] 树的最大深度。增加该值将使得模型更加复杂，更容易导致过拟合。0表示不限制输的深度。需要强调的是，当grow_policy设置为depthwise时，该限制是必须的。 大小区间: [0,∞] min_child_weight [default=1] 子节点所需的实例权重总和的最小值(hessian是什么意思？)。如果树的分割导致一个叶子节点的实例权重总和小于min_child_weight，那么将会停止进一步的分割。 在线性回归任务中，该参数对应于每个节点中需要的最小实例数。min_child_weight越大，算法越保守。 大小区间: [0,∞] max_delta_step [default=0] 我们允许每个叶子节点输出的最大步长增量。（该参数还不太理解，先附上原文）Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative. Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced. Set it to value of 1-10 might help control the update. 大小区间: [0,∞] subsample [default=1] 训练实例的子采样率。设置为0.5意味着在生成树之前，XGBoost将会随机抽取一半的训练数据。这将防止过拟合。子采样在每一次提升迭代时执行一次。 大小区间: (0,1] colsample_bytree [default=1] 构造每颗树时使用的列的子采样率。Subsample ratio of columns when constructing each tree. 子采样在每一次提升迭代时执行一次。 大小区间: (0,1] colsample_bylevel [default=1] 在每一个level，每一次分割时列的子采样率。每一次新的分割产生时子采样只会发生一次。当tree_method设置为hist时，该参数无效。 大小区间: (0,1] lambda [default=1, alias: reg_lambda] L2正则化项的权重。增加该参数的值将会使模型更加保守。 alpha [default=0, alias: reg_alpha] L1正则化项的权重。增加该参数的值将会使模型更加保守。 tree_method string [default= auto] XGBoost中树的构造算法。请在相关的论文中查看具体的描述。 分布式存储和外部存储器版本的仅支持tree_method=approx. 可选: auto, exact, approx, hist, gpu_exact, gpu_hist auto: 使用启发式方法选择最快算法。 对于中小型数据集，将使用精确贪婪算法exact greedy (exact) 对于超大型数据集，将使用近似算法approximate algorithm (approx) 由于过去在单个机器中总是使用exact greedy，当approximate algorithm被采用时，用户将获得提醒这一选择的信息。 exact: Exact greedy algorithm（精确贪婪算法）. approx: Approximate greedy algorithm using quantile sketch and gradient histogram. hist: Fast histogram optimized approximate greedy algorithm. It uses some performance improvements such as bins caching. gpu_exact: GPU implementation of exact algorithm. gpu_hist: GPU implementation of hist algorithm. sketch_eps [default=0.03] 当tree_method=approx，该参数才会被使用. This roughly translates into O(1 / sketch_eps) number of bins. Compared to directly select number of bins, this comes with theoretical guarantee with sketch accuracy. 通常用户不必调整该参数。可以考虑设置一个较小的值，可以更准确的遍历分割候选节点。 range: (0, 1) scale_pos_weight [default=1] 控制正负权重的平衡，对不平衡类有用（Control the balance of positive and negative weights, useful for unbalanced classes）. 考虑典型值：sum(negative instances) / sum(positive instances). 更多讨论，参见参数调整。另外可以查看Higgs的Kaggle竞赛demo for examples: R, py1, py2, py3. updater [default= grow_colmaker,prune] A comma separated string defining the sequence of tree updaters to run, providing a modular way to construct and to modify the trees. This is an advanced parameter that is usually set automatically, depending on some other parameters. However, it could be also set explicitly by a user. The following updater plugins exist: grow_colmaker: non-distributed column-based construction of trees. distcol: distributed tree construction with column-based data splitting mode. grow_histmaker: distributed tree construction with row-based data splitting based on global proposal of histogram counting. grow_local_histmaker: based on local histogram counting. grow_skmaker: uses the approximate sketching algorithm. sync: synchronizes trees in all distributed nodes. refresh: refreshes tree’s statistics and/or leaf values based on the current data. Note that no random subsampling of data rows is performed. prune: prunes the splits where loss &lt; min_split_loss (or gamma). In a distributed setting, the implicit updater sequence value would be adjusted to grow_histmaker,prune. refresh_leaf [default=1] This is a parameter of the refresh updater plugin. When this flag is 1, tree leafs as well as tree nodes’ stats are updated. When it is 0, only node stats are updated. process_type [default= default] A type of boosting process to run. Choices: default, update default: The normal boosting process which creates new trees. update: Starts from an existing model and only updates its trees. In each boosting iteration, a tree from the initial model is taken, a specified sequence of updater plugins is run for that tree, and a modified tree is added to the new model. The new model would have either the same or smaller number of trees, depending on the number of boosting iteratons performed. Currently, the following built-in updater plugins could be meaningfully used with this process type: refresh, prune. With process_type=update, one cannot use updater plugins that create new trees. grow_policy [default= depthwise] Controls a way new nodes are added to the tree. Currently supported only if tree_method is set to hist. Choices: depthwise, `lossguide depthwise: split at nodes closest to the root. lossguide: split at nodes with highest loss change. max_leaves [default=0] Maximum number of nodes to be added. Only relevant when grow_policy=lossguide is set. max_bin, [default=256] Only used if tree_method is set to hist. Maximum number of discrete bins to bucket continuous features. Increasing this number improves the optimality of splits at the cost of higher computation time. predictor, [default=cpu_predictor] The type of predictor algorithm to use. Provides the same results but allows the use of GPU or CPU. cpu_predictor: Multicore CPU prediction algorithm. gpu_predictor: Prediction using GPU. Default when tree_method is gpu_exact or gpu_hist. Learning Task Parameters指定learning task和相应的learning objective. objective选项如下: objective [default=reg:linear] reg:linear: linear regression reg:logistic: logistic regression binary:logistic: logistic regression for binary classification, output probability binary:logitraw: logistic regression for binary classification, output score before logistic transformation gpu:reg:linear, gpu:reg:logistic, gpu:binary:logistic, gpu:binary:logitraw: versions of the corresponding objective functions evaluated on the GPU; note that like the GPU histogram algorithm, they can only be used when the entire training session uses the same dataset count:poisson –poisson regression for count data, output mean of poisson distribution max_delta_step is set to 0.7 by default in poisson regression (used to safeguard optimization) survival:cox: Cox regression for right censored survival time data (negative values are considered right censored). Note that predictions are returned on the hazard ratio scale (i.e., as HR = exp(marginal_prediction) in the proportional hazard function h(t) = h0(t) * HR). multi:softmax: set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class(number of classes) multi:softprob: same as softmax, but output a vector of ndata nclass, which can be further reshaped to ndata nclass matrix. The result contains predicted probability of each data point belonging to each class. rank:pairwise: 设置XGBoost通过最小化成对损失（pairwise loss）来执行排名任务（ranking task）。 reg:gamma: gamma regression with log-link. Output is a mean of gamma distribution. It might be useful, e.g., for modeling insurance claims severity, or for any outcome that might be gamma-distributed. reg:tweedie: Tweedie regression with log-link. It might be useful, e.g., for modeling total loss in insurance, or for any outcome that might be Tweedie-distributed. base_score [default=0.5] The initial prediction score of all instances, global bias For sufficient number of iterations, changing this value will not have too much effect. eval_metric [default according to objective] 验证数据的评估指标，根据objective给出默认的指标（回归：rmse；分类：error；排名：mean average precision） 用户可以添加多个评估指标。对于Python用户：记得将指标作为参数对list而不是map进行传递，防止后面的eval_metric不会覆盖前一个。 可选项如下: rmse: root mean square error mae: mean absolute error logloss: negative log-likelihood error: Binary classification error rate. It is calculated as #(wrong cases)/#(all cases). For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances. error@t: a different than 0.5 binary classification threshold value could be specified by providing a numerical value through ‘t’. merror: Multiclass classification error rate. It is calculated as #(wrong cases)/#(all cases). mlogloss: Multiclass logloss. auc: Area under the curve ndcg: Normalized Discounted Cumulative Gain map: Mean average precision ndcg@n, map@n: ‘n’ can be assigned as an integer to cut off the top positions in the lists for evaluation. ndcg-, map-, ndcg@n-, map@n-: In XGBoost, NDCG and MAP will evaluate the score of a list without any positive samples as 1. By adding “-” in the evaluation metric XGBoost will evaluate these score as 0 to be consistent under some conditions. poisson-nloglik: negative log-likelihood for Poisson regression gamma-nloglik: negative log-likelihood for gamma regression cox-nloglik: negative partial log-likelihood for Cox proportional hazards regression gamma-deviance: residual deviance for gamma regression tweedie-nloglik: negative log-likelihood for Tweedie regression (at a specified value of the tweedie_variance_power parameter) seed [default=0] Random number seed.]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost Python Package中文文档]]></title>
    <url>%2F2018%2F07%2F20%2FUsing-xgboost-in-Python%2F</url>
    <content type="text"><![CDATA[XGBoost Python Package英文文档 安装XGBoost 安装引导文档 验证是否安装成功 1import xgboost as xgb 数据接口XGBoost的Python模块可以从以下几种文件导入数据： LibSVM text format file Comma-separated values (CSV) file NumPy 2D array SciPy 2D sparse array XGBoost binary buffer file 点击Text Input Format of DMatrix查看具体的text输入形式 这些数据存储在DMatrix对象中。 导入a libsvm text文件或者a XGBoost binary文件到DMatrix: 12dtrain = xgb.DMatrix('train.svm.txt')dtest = xgb.DMatrix('test.svm.buffer') 导入a CSV file到DMatrix: 123# label_column specifies the index of the column containing the true labeldtrain = xgb.DMatrix('train.csv?format=csv&amp;label_column=0')dtest = xgb.DMatrix('test.csv?format=csv&amp;label_column=0') (需要强调的是，XGBoost不支持categorical features; 如果你的数据包含categorical features, 先把它加载成a NumPy数组，然后进行独热编码.) 导入a NumPy array到DMatrix: 123data = np.random.rand(5, 10) # 5 entities, each contains 10 featureslabel = np.random.randint(2, size=5) # binary targetdtrain = xgb.DMatrix(data, label=label) 导入a scipy.sparse array到DMatrix: 12csr = scipy.sparse.csr_matrix((dat, (row, col)))dtrain = xgb.DMatrix(csr) 保存数据到a XGBoost binary文件可以提高导入速度: 12dtrain = xgb.DMatrix('train.svm.txt')dtrain.save_binary('train.buffer') 缺失值可以在DMatrix的构造函数中被默认值替代: 1dtrain = xgb.DMatrix(data, label=label, missing=-999.0) 在需要的时候可以设置权重： 12w = np.random.rand(5, 1)dtrain = xgb.DMatrix(data, label=label, missing=-999.0, weight=w) 设置参数XGBoost可以使用a list of pairs或者字典设置参数。例如： 提升器参数 123param = &#123;'max_depth': 2, 'eta': 1, 'silent': 1, 'objective': 'binary:logistic'&#125;param['nthread'] = 4param['eval_metric'] = 'auc' 你也可以指定多个指标： 12345param['eval_metric'] = ['auc', 'ams@0']# alternatively:# plst = param.items()# plst += [('eval_metric', 'ams@0')] 指定验证集查看模型性能 1evallist = [(dtest, 'eval'), (dtrain, 'train')] 训练训练一个模型需要参数列表和数据集。 12num_round = 10bst = xgb.train(param, dtrain, num_round, evallist) 训练之后，可以保存模型 1bst.save_model('0001.model') 模型和它的feature map也可以输出成text文件。 12345# dump modelbst.dump_model('dump.raw.txt')# dump model with feature mapbst.dump_model('dump.raw.txt', 'featmap.txt')` 上面保存的模型可以通过一下代码加载： 12bst = xgb.Booster(&#123;'nthread': 4&#125;) # init modelbst.load_model('model.bin') # load data 提前停止(Early Stopping)如果你有一个验证集，你可以使用early stopping寻找一个最优的提升次数。Early Stopping需要evals中至少有一个验证集。如果evals中的验证集多于一个，将会使用最后一个。 1train(..., evals=evals, early_stopping_rounds=10) 模型将会一直训练直到验证集的score不再提升。验证集误差需要再每early_stopping_rounds轮中保持减少，模型才会继续训练。 如果提前停止发生了，模型将会有三个额外的fields：bst.best_score, bst.best_iteration and bst.best_ntree_limit。需要强调的是，xgboost.train()将会返回最后一次迭代的模型，而不是最好的一个。 这适用于两种指标：最小化（RMSE，log loss等）和最大化（MAP，NDCG，AUC）。需要强调的是，如果你在param[‘eval_metric’]中指定了多种评价指标，最后一个指标将会被用到early stopping中。 预测训练好或者加载的模型可以预测数据集。 1234# 7 entities, each contains 10 featuresdata = np.random.rand(7, 10)dtest = xgb.DMatrix(data)ypred = bst.predict(dtest) 如果在训练的时候开启了提前停止功能，你可以通过bst.best_ntree_limit获得最优迭代次数下的预测值。 1ypred = bst.predict(dtest, ntree_limit=bst.best_ntree_limit) 绘图你可以使用plotting模块对importance and output tree进行绘图展示。 绘制importance，可以使用xgboost.plot_importance()。该函数需要安装依赖库matplotlib。 1xgb.plot_importance(bst) 通过matplotlib绘制输出树，可以使用xgboost.plot_tree()，需要指定目标树的序号。这个函数需要依赖库graphviz和matplotlib。 1xgb.plot_tree(bst, num_trees=2) 当你使用IPython，你可以使用xgboost.to_graphviz()函数；该函数将目标树转换成一个graphviz实例。这个graphviz实例将在IPython中自动被渲染。 1xgb.to_graphviz(bst, num_trees=2)]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据竞赛要点]]></title>
    <url>%2F2018%2F07%2F18%2F%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B%E8%A6%81%E7%82%B9%2F</url>
    <content type="text"><![CDATA[视频链接：从0-1的数据竞赛经验分享 0 所需能力0.1 工具 语言：Python 库：Pandas, Numpy, Sklearn, Scipy, Seaborn, Keras, Xgboost, Lightgbm 0.2 参考书籍 李航《统计学习方法》 周老师《机器学习》 1 建模工作框架 2 数据预处理删除噪音，获得更加干净的数据 2.1缺失值处理 缺失严重（达到90%）：直接删除特征列 单条记录缺失严重（达到90%）：直接删除记录 缺失不严重（低于90%）：填充均值、中位数、单独生成一列0-1，或根据相关性高的特征进行还原 2.2 奇异值处理 转化为非奇异值 无法修整，直接删除 区分标签与特征的处理 2.3 特殊的预处理 流量的归一化 图像的标准化 3 特征工程3.1 人工特征：人为构建，基于个人水平和经验 覆盖面越多越好 覆盖角度越多越好（宏观到微观） 与预测目标相关的所有信息全部加入 3.2 机器特征：模型的生成，模型的理解 PCA，FLD特征 GBDT输出的路径特征 神经网络特征（AutoEncoder等） 3.3 自动化特征轮：AutoML（流行），可根据经验自行设计4 模型逼近上界 4.1 最为流行的模型4.1.1 单模型 结构化数据类：XGBoost，LightGBM等 推荐类：FFM等 图像+文本类：各种神经网络 4.1.2 模型融合 均值集成，加权集成，Rank集成 Stacking：简单的5折Stacking，StackNet等 4.2 如何让模型更好的消化数据？目前80%~90%的比赛冠军方案都是基于LGB，XGB，RF，GBDT模型 4.2.1 反作用数据预处理 数据过少不具有代表性的：删除 方差较小不具有代表性的：删除 4.2.2 反作用特征工程 一阶、二阶、三阶+固定属性特征 不同的比赛略有不同 5 数据分析一般质的飞跃都在这个阶段 数据标签分析：好的label构建，成功了一半 数据特征分析： a) 设计更好的特征 b) 设计强特 结果分析 a) 根据预测结果设计Tricks b) 李勇预测结果设计更为高级的算法，例如：基于RF概率的KNN修正 一个完整的机器学习项目的流程 把实际问题抽象成一个数学问题，确定机器学习的目标，即分类or回归or聚类 获取数据 数据本身：具有代表性 类别问题：避免类别不平衡问题 dataSize：数据太大或者特征太多，可以考虑使用分布式处理数据 数据预处理 归一化 标准化 因子化 离散化 缺失值处理 去除共线性 数据集探索与数据可视化 探索、分析数据，可视化是重要的工具 特征工程与特征工程 特征工程：提取特征 特征选择：筛选特征 特征有效性分析技术 相关系数 卡方检验 平均互信息 条件熵 后验概率 逻辑回归权重 选择模型，训练模型，参数调优 sklearn中封装了常用的传统机器学习方法，根据我们学习的任务内容，选择合适的模型 （超）参数调优是难点，需要对使用的机器学习方法有很好的理解 模型诊断 常使用交叉验证，绘制学习曲线的方法判断是否过拟合or欠拟合 过拟合：增加数据，减少模型的复杂度 欠拟合：提高数据集特征的数量，增加模型的复杂度 误差分析：观察误差样本 -&gt; 分析原因 -&gt; 参数原因or模型原因or特征原因or数据本身 模型融合 有一定的提升效果 工程上提升准确度主要通过：（标准可复制、效果稳定） 模型的前端，即数据预处理、特征工程、数据清洗 模型的后端，即模型融合 调参使用较少，数据量大难以GridSearch 模型上线运行 需要考虑的因素有： 准确度 误差 时间复杂度 空间复杂度 稳定性]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>Kaggle</tag>
        <tag>天池</tag>
        <tag>数据竞赛</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑斯谛回归模型]]></title>
    <url>%2F2018%2F07%2F11%2FLogitstic-Regression%2F</url>
    <content type="text"><![CDATA[逻辑斯谛分布X是连续随机变量，其分布函数和密度函数为 $$F(x) = P(X &lt;= x) = \frac{1}{1+e^{-(x-\mu)/\gamma}}$$ $$f(x) = F’(x) = {\frac{e^{-(x-\mu)/\gamma}}{\gamma(1+e^{-(x-\mu)/\gamma})}}$$ 式中：$\mu$是位置参数，$\gamma$是形状参数 其分布函数即为逻辑斯谛函数，其图形为S形曲线（sigmoid curve），该曲线以（$\mu$,1/2）中心对称，满足 $$F(-x+\mu) - 1/2 = -F(x-\mu) + 1/2$$ 二项逻辑斯谛回归模型$$ P(Y=1|x) = \frac{exp(w \cdot x)}{1+exp(w \cdot x)}$$$$ P(Y=0|x) = \frac{1}{1+exp(w \cdot x)}$$ 式中：$x = (x^{(1)}, x^{(2)}, \dots, x^{(n)}, 1)^T$，$Y = (w^{(1)}, w^{(2)}, \dots, w^{(n)}, b)^T$, 其中$w$是权值向量，$b$为偏置。 逻辑斯谛回归模型的特点为：输出Y=1的对数几率是由输入x的线性函数表示的模型，即为逻辑斯谛回归模型 对数几率 $$ logit(P) = \log \frac{p}{1-p} $$ $$ logit(P(Y = 1|x)) = \log \frac{P(Y = 1|x)}{1 - P(Y = 1| x)} = w \cdot x $$ 即逻辑斯谛回归模型的对数几率是x的线性函数！！！ 模型参数估计训练数据集$D = {(x_1, y_1), (x_2, y_2), \dots, (x_N, y_N)}$，其中，$x_i \in R^n, y_i \in {0, 1}$ 估计方法极大似然法 过程设：$P(Y=1|x) = \pi(x), P(Y=0|x) = 1 - \pi(x)$ 似然函数 $$\prod^N_{i=1} [\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$$ 对数似然函数 $$L(w) = \sum^N_{i=1} [y_i\log\pi(x_i) + (1-y_i)\log(1 - \pi(x_i))] = \sum^N_{i=1} [y_i\log\frac{\pi(x_i)}{1 - \pi(x_i)} + \log(1 - \pi(x_i))]$$ 根据逻辑斯谛回归模型的对数几率特性 $$L(w) = \sum^N_{i=1} [y_i(w \cdot x_i) - \log (1 + exp(w \cdot x_i))]$$ 对L(w)求取极大值得到$w$的估计值$\widehat{w}$]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>Logitsic Regression</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[修改Mysql的数据库存储路径]]></title>
    <url>%2F2018%2F07%2F04%2Fmodify-the-datadir-of-mysql%2F</url>
    <content type="text"><![CDATA[Windows7下修改Mysql的数据库存储路径查看Mysql默认的数据库存储路径 打开MYSQL 5.7 Command Line Client 输入命令 1show global variables like &quot;%datadir%&quot;; 默认的数据库存储路径为：C:\ProgramData\MySQL\MySQL Server 5.7\Data\ 停止Mysql服务创建新的数据库存放目录 我的数据库存放目录为：F:\ProgramData\MySQL\MySQL Server 5.7\Data\ 修改my.ini my.ini的路径为：C:\ProgramData\MySQL\MySQL Server 5.7\my.ini 打开后找到datadir= 修改为datadir=F:\ProgramData\MySQL\MySQL Server 5.7\Data 重启Mysql服务]]></content>
      <categories>
        <category>数据库基础</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[牛客网华为机试：Using Python]]></title>
    <url>%2F2018%2F06%2F29%2FPython-written-examinatioon%2F</url>
    <content type="text"><![CDATA[第一题 题目描述计算字符串最后一个单词的长度，单词以空格隔开。 输入描述一行字符串，非空，长度小于5000。 输出描述整数N，最后一个单词的长度。 示例1输入：hello world 输出：5 12345678910def function(): a = input() if &apos; &apos; not in a: print(len(a)) else: sp = a.split(&apos; &apos;) print(len(sp[-1]))function() 第二题 题目描述写出一个程序，接受一个有字母和数字以及空格组成的字符串，和一个字符，然后输出输入字符串中含有该字符的个数。不区分大小写。 输入描述输入一个有字母和数字以及空格组成的字符串，和一个字符。 输出描述输出输入字符串中含有该字符的个数。 示例1输入：ABCDEF A 输出：1 12345678910111213141516171819def function(): a = input() b = input() count = 0 if (ord(b) &gt;= 97) and (ord(b) &lt;= 122): for el in a: if (ord(el) == ord(b)) or (ord(el) == (ord(b) - 32)): count += 1 elif (ord(b) &gt;= 65) and (ord(b) &lt;= 90): for el in a: if (ord(el) == ord(b)) or (ord(el) == (ord(b) + 32)): count += 1 else: for el in a: if ord(el) == ord(b): count += 1 print(count)function() 第三题 题目描述明明想在学校中请一些同学一起做一项问卷调查，为了实验的客观性，他先用计算机生成了N个1到1000之间的随机整数（N≤1000），对于其中重复的数字，只保留一个，把其余相同的数去掉，不同的数对应着不同的学生的学号。然后再把这些数从小到大排序，按照排好的顺序去找同学做调查。请你协助明明完成“去重”与“排序”的工作。 1234567891011Input Param n 输入随机数的个数 inputArray n个随机整数组成的数组Return Value OutputArray 输出处理后的随机整数注：测试用例保证输入参数的正确性，答题者无需验证。测试用例不止一组。 输入描述输入多行，先输入随机整数的个数，再输入相应个数的整数 输出描述返回多行，处理后的结果 示例1 1234567891011121314151617181920212223242526输入：-----11102040326740208930040015输出：-----10152032406789300400 123456789101112while True: try: n = int(input()) inputArray = [] for i in range(n): inputArray.append(int(input())) outputArray = list(set(inputArray)) outputArray.sort() for el in outputArray: print(el) except: break 第四题 题目描述•连续输入字符串，请按长度为8拆分每个字符串后输出到新的字符串数组；•长度不是8整数倍的字符串请在后面补数字0，空字符串不处理。 输入描述连续输入字符串(输入2次,每个字符串长度小于100) 输出描述输出到长度为8的新字符串数组 示例1 12345678910输入：-----abc123456789输出：-----abc000001234567890000000 12345678910111213141516171819202122232425262728293031323334while True: try: str1 = input() if str1: length1 = len(str1) if (length1 % 8) == 0: for i in range(0, length1, 8): print(str1[i:i + 8]) else: length1_ = (length1 // 8) * 8 + 8 for i in range(length1, length1_): str1 += &apos;0&apos; for i in range(0, length1_, 8): print(str1[i:i + 8]) else: print(str1) str2 = input() if str2: length2 = len(str2) if (length2 % 8) == 0: for i in range(0, length2, 8): print(str2[i:i + 8]) else: length2_ = (length2 // 8) * 8 + 8 for i in range(length2, length2_): str2 += &apos;0&apos; for i in range(0, length2_, 8): print(str2[i:i + 8]) else: print(str2) except: break 第五题 题目描述写出一个程序，接受一个十六进制的数值字符串，输出该数值的十进制字符串。（多组同时输入 ） 输入描述输入一个十六进制的数值字符串。 输出描述输出该数值的十进制字符串。 示例1 1234567输入：-----0xA输出：-----10 123456while True: try: Hex = input() print(int(Hex, 16)) except: break 第六题 题目描述功能:输入一个正整数，按照从小到大的顺序输出它的所有质数的因子（如180的质数因子为2 2 3 3 5 ）最后一个数后面也要有空格 详细描述函数接口说明：public String getResult(long ulDataInput)输入参数：long ulDataInput：输入的正整数返回值：String 输入描述输入一个long型整数 输出描述按照从小到大的顺序输出它的所有质数的因子，以空格隔开。最后一个数后面也要有空格。 示例1 1234567输入：-----180输出：-----2 2 3 3 5 12345678910111213while True: try: num = int(input()) while num &gt; 1: i = 2 while True: if num % i == 0: print(i, end=&apos; &apos;) num /= i break i += 1 except: break 第七题 题目描述写出一个程序，接受一个正浮点数值，输出该数值的近似整数值。如果小数点后数值大于等于5,向上取整；小于5，则向下取整。 输入描述输入一个正浮点数值 输出描述输出该数值的近似整数值 示例11234567输入：-----5.5输出：-----6 12345678910while True: try: num = input() index = num.index(&apos;.&apos;) if int(num[index+1]) &gt;= 5: print(int(num[: index])+1) else: print(int(num[: index])) except: break 第八题 题目描述数据表记录包含表索引和数值，请对表索引相同的记录进行合并，即将相同索引的数值进行求和运算，输出按照key值升序进行输出。 输入描述先输入键值对的个数然后输入成对的index和value值，以空格隔开 输出描述输出合并后的键值对（多行） 示例112345678910111213输入：-----40 10 21 23 4输出：----0 31 23 4 123456789101112while True: try: num = int(input()) output = &#123;&#125; for i in range(num): pair = input().split(&apos; &apos;) output[int(pair[0])] = output.setdefault(int(pair[0]), 0) + int(pair[1]) output_list = sorted(output.keys()) for i in output_list: print(i, output[i]) except: break 第九题 题目描述输入一个int型整数，按照从右向左的阅读顺序，返回一个不含重复数字的新的整数。 输入描述输入一个int型整数 输出描述按照从右向左的阅读顺序，返回一个不含重复数字的新的整数 示例1 输入：9876673 输出：37689 1234567891011while True: try: num = input() length = len(num) output = num[-1] for i in range(length-2, -1, -1): if num[i] not in num[i+1:]: output += num[i] print(output) except: break 第十题 题目描述输入一个整数，将这个整数以字符串的形式逆序输出 程序不考虑负数的情况，若数字含有0，则逆序形式也含有0，如输入为100，则输出为001 输入描述输入一个int整数 输出描述将这个整数以字符串的形式逆序输出 示例1 输入：1516000 输出：0006151 12345while True: try: print(input()[::-1]) except: break 第十一题 题目描述写出一个程序，接受一个字符串，然后输出该字符串反转后的字符串。 输入描述输入N个字符 输出描述输出该字符串反转后的字符串 示例1 输入：abcd 输出：dcba 12345while True: try: print(input()[::-1]) except: break 第十二题 题目描述将一个英文语句以单词为单位逆序排放。例如“I am a boy”，逆序排放后为“boy a am I”，所有单词之间用一个空格隔开，语句中除了英文字母外，不再包含其他字符 输入描述将一个英文语句以单词为单位逆序排放。 输出描述得到逆序的句子 示例1 输入：I am a boy 输出：boy a am I 123456789101112while True: try: sen = input().split(' ') s = '' for index, word in enumerate(sen[:: -1]): if index == (len(sen) -1): s += word else: s = s + word + ' ' print(s) except: break 第十三题 字串的连接最长路径查找 题目描述给定n个字符串，请对n个字符串按照字典序排列。 输入描述输入第一行为一个正整数n(1≤n≤1000),下面n行为n个字符串(字符串长度≤100),字符串中只含有大小写字母。 输出描述数据输出n行，输出结果为按照字典序排列的字符串。 示例1 输入： 9 cap to cat card two too up boat boot 输出： boat boot cap card cat to too two up 1234567891011while True: try: num = int(input()) strings = [] for i in range(num): strings.append(input()) strings.sort() for i in strings: print(i) except: break 第十四题 求int型正整数在内存中存储时1的个数 题目描述输入一个int型的正整数，计算出该int型数据在内存中存储时1的个数。 输入描述输入一个整数（int类型） 输出描述这个数转换成2进制后，输出1的个数 示例1 输入： 5 输出： 2 12345678910while True: try: num = int(input()) count = 0 for i in bin(num): if i == &apos;1&apos;: count += 1 print(count) except: break 第十五题 购物单 题目描述王强今天很开心，公司发给N元的年终奖。王强决定把年终奖用于购物，他把想买的物品分为两类：主件与附件，附件是从属于某个主件的，下表就是一些主件与附件的例子： 主件 附件 电脑 打印机，扫描仪 书柜 图书 书桌 台灯，文具 工作椅 无 如果要买归类为附件的物品，必须先买该附件所属的主件。每个主件可以有 0 个、 1 个或 2 个附件。附件不再有从属于自己的附件。王强想买的东西很多，为了不超出预算，他把每件物品规定了一个重要度，分为 5 等：用整数 1 ~ 5 表示，第 5 等最重要。他还从因特网上查到了每件物品的价格（都是 10 元的整数倍）。他希望在不超过 N 元（可以等于 N 元）的前提下，使每件物品的价格与重要度的乘积的总和最大。 设第 j 件物品的价格为 v[j] ，重要度为 w[j] ，共选中了 k 件物品，编号依次为 j 1 ， j 2 ，……， j k ，则所求的总和为：v[j 1 ]w[j 1 ]+v[j 2 ]w[j 2 ]+ … +v[j k ]w[j k ] 。（其中 为乘号） 请你帮助王强设计一个满足要求的购物单。 输入描述输入的第 1 行，为两个正整数，用一个空格隔开：N m （其中 N （ &lt;32000 ）表示总钱数， m （ &lt;60 ）为希望购买物品的个数。） 从第 2 行到第 m+1 行，第 j 行给出了编号为 j-1 的物品的基本数据，每行有 3 个非负整数 v p q （其中 v 表示该物品的价格（ v0 ，表示该物品为附件， q 是所属主件的编号） 输出描述输出文件只有一个正整数，为不超过总钱数的物品的价格与重要度乘积的总和的最大值（ &lt;200000 ）。 示例1 输入： 1000 5 800 2 0 400 5 1 300 5 1 400 3 0 500 2 0 输出： 2200 12345678910111213141516171819while True: try: N, m = map(int, input().strip().split()) goods = [] for i in range(m): goods.append(list(map(int, input().strip().split()))) def dp_max(N, m, goods): a = [[0]*(N+1) for _ in range(m+1)] for i in range(1, m+1): for j in range(10, N+1, 10): if goods[i-1][2] == 0: if goods[i-1][0] &lt;= j: a[i][j] = max(a[i-1][j], a[i-1][j - goods[i-1][0]] + goods[i-1][0]*goods[i-1][1]) elif (goods[i-1][0] + goods[goods[i-1][2]-1][0]) &lt;= j: a[i][j] = max(a[i-1][j], a[i-1][j - goods[i-1][0] - goods[goods[i-1][2]-1][0]] + goods[i-1][0] * goods[i-1][1] + goods[goods[i-1][2]-1][0] * goods[goods[i-1][2]-1][1]) print(a[m][int(N / 10) * 10]) dp_max(N, m, goods) except: break 第十六题 坐标移动 题目描述开发一个坐标计算工具， A表示向左移动，D表示向右移动，W表示向上移动，S表示向下移动。从（0,0）点开始移动，从输入字符串里面读取一些坐标，并将最终输入结果输出到输出文件里面。 123456789101112131415161718输入：合法坐标为A(或者D或者W或者S) + 数字（两位以内）坐标之间以;分隔。非法坐标点需要进行丢弃。如AA10; A1A; $%$; YAD; 等。下面是一个简单的例子 如：A10;S20;W10;D30;X;A1A;B10A11;;A10;处理过程：起点（0,0）+ A10 = （-10,0）+ S20 = (-10,-20)+ W10 = (-10,-10)+ D30 = (20,-10)+ x = 无效+ A1A = 无效+ B10A11 = 无效+ 一个空 不影响+ A10 = (10,-10)结果 （10， -10） 输入描述一行字符串 输出描述最终坐标，以,分隔 示例1 输入：A10;S20;W10;D30;X;A1A;B10A11;;A10; 输出：10,-10 123456789101112131415161718192021import sysimport refor line in sys.stdin: text = line.strip().split(&apos;;&apos;) x = 0 y = 0 actions = [] for el in text: if re.match(&apos;^[AWDS]\d$&apos;, el) or re.match(&apos;^[AWDS]\d\d$&apos;, el): actions.append(el) for el in actions: if el[0] == &apos;A&apos;: x -= int(el[1:]) if el[0] == &apos;D&apos;: x += int(el[1:]) if el[0] == &apos;W&apos;: y += int(el[1:]) if el[0] == &apos;S&apos;: y -= int(el[1:]) print(&apos;%d,%d&apos; % (x, y)) 第十七题 识别有效的IP地址和掩码并进行分类统计 题目描述请解析IP地址和对应的掩码，进行分类识别。要求按照A/B/C/D/E类地址归类，不合法的地址和掩码单独归类。 所有的IP地址划分为 A,B,C,D,E五类 A类地址1.0.0.0~126.255.255.255; B类地址128.0.0.0~191.255.255.255; C类地址192.0.0.0~223.255.255.255; D类地址224.0.0.0~239.255.255.255； E类地址240.0.0.0~255.255.255.255 私网IP范围是： 10.0.0.0～10.255.255.255 172.16.0.0～172.31.255.255 192.168.0.0～192.168.255.255 子网掩码为前面是连续的1，然后全是0。（例如：255.255.255.32就是一个非法的掩码）本题暂时默认以0开头的IP地址是合法的，比如0.1.1.2，是合法地址 输入描述多行字符串。每行一个IP地址和掩码，用~隔开。 输出描述统计A、B、C、D、E、错误IP地址或错误掩码、私有IP的个数，之间以空格隔开。 示例1 输入： 0.70.44.68~255.254.255.0 1.0.0.1~255.0.0.0 192.168.0.2~255.255.255.0 19..0.~255.255.255.0 输出：1 0 1 0 0 2 1 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import sysA = 0B = 0C = 0D = 0E = 0err = 0pri = 0legal = ['254', '252', '248', '240', '224', '192', '128', '0']def check_ip(ip): if (len(ip) != 4) or ('' in ip): return False else: for i in ip: if (int(ip[0]) &lt; 0) or (int(ip[0]) &gt; 255): return False else: return Truedef check_mask(ms): if (len(ms) != 4) or ('' in ms): return False if ms[0] == '255': if ms[1] == '255': if ms[2] == '255': if ms[3] in legal: return True else: return False elif (ms[2] in legal) and (ms[3] == '0'): return True else: return False elif (ms[1] in legal) and (ms[2] == '0') and (ms[3] == '0'): return True else: return False elif (ms[0] in legal) and (ms[1] == '0') and (ms[2] == '0') and (ms[3] == '0'): return True else: return Falsefor line in sys.stdin: elements = line.strip().split('\n') for el in elements: ip = el.split('~')[0].split('.') ms = el.split('~')[1].split('.') if check_ip(ip) and check_mask(ms): if (1 &lt;= int(ip[0])) and (int(ip[0]) &lt;= 126): A += 1 if (128 &lt;= int(ip[0])) and (int(ip[0]) &lt;= 191): B += 1 if (192 &lt;= int(ip[0])) and (int(ip[0]) &lt;= 223): C += 1 if (224 &lt;= int(ip[0])) and (int(ip[0]) &lt;= 239): D += 1 if (240 &lt;= int(ip[0])) and (int(ip[0]) &lt;= 255): E += 1 if (int(ip[0]) == 10) or ((int(ip[0]) == 172) and (16 &lt;= int(ip[1])) and (int(ip[1]) &lt;= 31)) or ((int(ip[0]) == 192) and (int(ip[1]) == 168)): pri += 1 else: err += 1print('%d %d %d %d %d %d %d' % (A, B, C, D, E, err, pri)) 第十八题 简单错误记录 题目描述开发一个简单错误记录功能小模块，能够记录出错的代码所在的文件名称和行号。 处理: 1、 记录最多8条错误记录，循环记录，对相同的错误记录（净文件名称和行号完全匹配）只记录一条，错误计数增加； 2、 超过16个字符的文件名称，只记录文件的最后有效16个字符； 3、 输入的文件可能带路径，记录文件名称不能带路径。 输入描述一行或多行字符串。每行包括带路径文件名称，行号，以空格隔开。 输出描述将所有的记录统计并将结果输出，格式：文件名 代码行数 数目，一个空格隔开，如： 示例1 输入：E:\V1R2\product\fpgadrive.c 1325 输出：fpgadrive.c 1325 1 123456789101112131415161718import sysrecords = []count = &#123;&#125;for line in sys.stdin: record, num = line.strip().split() if &apos;\\&apos; in record: record = record.split(&apos;\\&apos;)[-1][-16:] else: record = record[-16:] record = record + &apos; &apos; + num if record not in records: records.append(record) count[record] = 1 else: count[record] += 1for record in records[-8:]: print(&apos;%s %d&apos; % (record, count[record])) 第十九题 密码验证合格程序 题目描述密码要求: 1.长度超过8位 2.包括大小写字母.数字.其它符号,以上四种至少三种 3.不能有相同长度超2的子串重复 说明:长度超过2的子串 输入描述一组或多组长度超过2的子符串。每组占一行 输出描述如果符合要求输出：OK，否则输出NG 示例1 输入: 021Abc9000 021Abc9Abc1 021ABC9000 021$bc9000 输出： OK NG NG OK 12345678910111213141516171819202122232425262728293031323334353637383940import sysdef check1(s): if len(s) &gt; 8: return True else: return False def check2(s): k1 = 0 k2 = 0 k3 = 0 k4 = 0 for el in s: if 'A' &lt;= el &lt;= 'Z': k1 = 1 elif 'a' &lt;= el &lt;= 'z': k2 = 1 elif '0' &lt;= el &lt;= '9': k3 = 1 else: k4 = 1 if (k1 + k2 + k3 + k4) &gt;= 3: return True else: return False def check3(s): for i in range(len(s) - 3): if s[i:i+3] in s[i+1:]: return False else: return Truefor line in sys.stdin: code = line.strip('\n') if check1(code) and check2(code) and check3(code): print('OK') else: print('NG') 第二十题 题目描述 输入描述 输出描述 示例1 第二十一题 题目描述 输入描述 输出描述 示例1]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>笔试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Anaconda安装Tensorflow框架]]></title>
    <url>%2F2018%2F06%2F27%2Finstall-tensorflow-in-Anaconda%2F</url>
    <content type="text"><![CDATA[1. conda清华源打开cmd，输入以下两条命令 12conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --set show_channel_urls yes 2. 安装Tensorflow2.1 创建名为tensorflow的虚拟环境12# Python 3.6conda create -n tensorflow python=3.6 2.2 激活tensorflow虚拟环境1activate tensorflow 2.3 安装tensorflow框架 安装指令 1conda install tensorflow 安装结果 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667(tensorflow) F:\&gt;conda install tensorflowSolving environment: done## Package Plan ## environment location: D:\Users\Administrator\Anaconda3\envs\tensorflow added / updated specs: - tensorflowThe following packages will be downloaded: package | build ---------------------------|----------------- tensorflow-1.2.1 | py36_0 21.0 MB https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free libprotobuf-3.2.0 | vc14_0 9.1 MB https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free bleach-1.5.0 | py36_0 22 KB https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free html5lib-0.9999999 | py36_0 178 KB https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free backports-1.0 | py36_0 3 KB https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free werkzeug-0.12.2 | py36_0 435 KB https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free protobuf-3.2.0 | py36_0 459 KB https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free zlib-1.2.11 | vc14_0 119 KB https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free markdown-2.6.9 | py36_0 100 KB https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free mkl-2017.0.3 | 0 126.3 MB https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free backports.weakref-1.0rc1 | py36_0 8 KB https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free numpy-1.13.1 | py36_0 3.6 MB https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free six-1.10.0 | py36_0 20 KB https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free ------------------------------------------------------------ Total: 161.4 MBThe following NEW packages will be INSTALLED: backports: 1.0-py36_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free backports.weakref: 1.0rc1-py36_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free bleach: 1.5.0-py36_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free html5lib: 0.9999999-py36_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free libprotobuf: 3.2.0-vc14_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free markdown: 2.6.9-py36_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free mkl: 2017.0.3-0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free numpy: 1.13.1-py36_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free protobuf: 3.2.0-py36_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free six: 1.10.0-py36_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free tensorflow: 1.2.1-py36_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free werkzeug: 0.12.2-py36_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free zlib: 1.2.11-vc14_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freeProceed ([y]/n)? yDownloading and Extracting Packagestensorflow-1.2.1 | 21.0 MB | ############################################################################################################### | 100%libprotobuf-3.2.0 | 9.1 MB | ############################################################################################################### | 100%bleach-1.5.0 | 22 KB | ############################################################################################################### | 100%html5lib-0.9999999 | 178 KB | ############################################################################################################### | 100%backports-1.0 | 3 KB | ############################################################################################################### | 100%werkzeug-0.12.2 | 435 KB | ############################################################################################################### | 100%protobuf-3.2.0 | 459 KB | ############################################################################################################### | 100%zlib-1.2.11 | 119 KB | ############################################################################################################### | 100%markdown-2.6.9 | 100 KB | ############################################################################################################### | 100%mkl-2017.0.3 | 126.3 MB | ############################################################################################################## | 100%backports.weakref-1. | 8 KB | ############################################################################################################### | 100%numpy-1.13.1 | 3.6 MB | ############################################################################################################### | 100%six-1.10.0 | 20 KB | ############################################################################################################### | 100%Preparing transaction: doneVerifying transaction: doneExecuting transaction: done]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
        <tag>Anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[皮尔森相关系数及其Seaborn可视化]]></title>
    <url>%2F2018%2F06%2F07%2FPearson-Correlation-Coefficient-and-Visualized-by-Seaborn%2F</url>
    <content type="text"><![CDATA[在统计学中，Pearson Correlation Coefficient(PCC)是对两个变量X和Y之间线性关系的量测。 定义 公式1 如果存在一个数据集{$x_1,x_2,…,x_n$}和另一个数据集{$y_1,y_2,…,y_n$}，这两个数据集都有n个变量，那么皮尔森相关系数为 $$r=\frac{\sum_{i=1}^{n} (x_i-\widetilde{x})(y_i-\widetilde{y})}{\sqrt{\sum_{i=1}^{n} (x_i-\widetilde{x})^2 } \sqrt{\sum_{i=1}^{n}(y_i-\widetilde{y})^2}}$$ 式中：n为样本规模；$\widetilde{x}=\frac{1}{n}\sum_{i=1}^{n}x_i$为样本均值，$\widetilde{y}$同理。 Seaborn可视化皮尔森相关系数热图1234567import seaborn as snsimport matplotlib.pyplot as pltcolormap = plt.cm.RdBuplt.figure(figsize=(14, 12))plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15)sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor=&apos;white&apos;, annot=True) pandas.DataFrame.corrDataFrame.corr(method=&apos;pearson&apos;, min_periods=1) 计算列之间的成对相关性，不包含NA/null值 Parameters: method:{&apos;pearson&apos;, &apos;kendall&apos;, &apos;spearman&apos;} - pearson : standard correlation coefficient - kendall : Kendall Tau correlation coefficient - spearman : Spearman rank correlation min_periods : int, optional Minimum number of observations required per pair of columns to have a valid result. Currently only available for pearson and spearman correlation Returns: y:DataFrame seaborn.heatmapseaborn.heatmap(data, vmin=None, vmax=None, cmap=None, center=None, robust=False, annot=None, fmt=&apos;.2g&apos;, annot_kws=None, linewidths=0, linecolor=&apos;white&apos;, cbar=True, cbar_kws=None, cbar_ax=None, square=False, xticklabels=&apos;auto&apos;, yticklabels=&apos;auto&apos;, mask=None, ax=None, **kwargs) 将矩形数据绘制为颜色编码矩阵 Parameters： data : rectangular dataset 2D dataset that can be coerced into an ndarray. If a Pandas DataFrame is provided, the index/column information will be used to label the columns and rows. vmin, vmax : floats, optional Values to anchor the colormap, otherwise they are inferred from the data and other keyword arguments. cmap : matplotlib colormap name or object, or list of colors, optional The mapping from data values to color space. If not provided, the default will depend on whether center is set. center : float, optional The value at which to center the colormap when plotting divergant data. Using this parameter will change the default cmap if none is specified. robust : bool, optional If True and vmin or vmax are absent, the colormap range is computed with robust quantiles instead of the extreme values. annot : bool or rectangular dataset, optional If True, write the data value in each cell. If an array-like with the same shape as data, then use this to annotate the heatmap instead of the raw data. fmt : string, optional String formatting code to use when adding annotations. annot_kws : dict of key, value mappings, optional Keyword arguments for ax.text when annot is True. linewidths : float, optional Width of the lines that will divide each cell. linecolor : color, optional Color of the lines that will divide each cell. cbar : boolean, optional Whether to draw a colorbar. cbar_kws : dict of key, value mappings, optional Keyword arguments for fig.colorbar. cbar_ax : matplotlib Axes, optional Axes in which to draw the colorbar, otherwise take space from the main Axes. square : boolean, optional If True, set the Axes aspect to “equal” so each cell will be square-shaped. xticklabels, yticklabels : “auto”, bool, list-like, or int, optional If True, plot the column names of the dataframe. If False, don’t plot the column names. If list-like, plot these alternate labels as the xticklabels. If an integer, use the column names but plot only every n label. If “auto”, try to densely plot non-overlapping labels. mask : boolean array or DataFrame, optional If passed, data will not be shown in cells where mask is True. Cells with missing values are automatically masked. ax : matplotlib Axes, optional Axes in which to draw the plot, otherwise use the currently-active Axes. kwargs : other keyword arguments All other keyword arguments are passed to ax.pcolormesh. Returns: ax : matplotlib Axes Axes object with the heatmap matplotlib.pyplot.cm.RdBumatplotlib.pyplot.cm.RdBu Seaborn.heatmap()中的cmap，热图中的矩形颜色]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>Pearson Correlation Coefficient</tag>
        <tag>皮尔森相关系数</tag>
        <tag>Seaborn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Titanic获救预测数据集预处理]]></title>
    <url>%2F2018%2F06%2F03%2FTitanic%E8%8E%B7%E6%95%91%E9%A2%84%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E9%A2%84%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[引言我的博客Titanic获救预测中对dataset的预处理感觉不是很完善，看了Kaggle上的一些Kernels，重新进行预处理（for 深度学习）… 特征处理12345678%matplotlib inlineimport pandas as pdimport numpy as npimport retrain = pd.read_csv(r'E:\Mirror\GitHub\Predict-survival-on-the-Titanic\data\train.csv')test = pd.read_csv(r'E:\Mirror\GitHub\Predict-survival-on-the-Titanic\data\test.csv')full_data = [train, test]train.info() &lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): PassengerId 891 non-null int64 Survived 891 non-null int64 Pclass 891 non-null int64 Name 891 non-null object Sex 891 non-null object Age 714 non-null float64 SibSp 891 non-null int64 Parch 891 non-null int64 Ticket 891 non-null object Fare 891 non-null float64 Cabin 204 non-null object Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.6+ KB 1. Pclass票类：经济地位的象征 序号 票类 1 头等舱 2 中等舱 3 末等舱 12345678910# One-hot编码# traintrain['P1'] = np.array(train['Pclass'] == 1).astype(np.int32)train['P2'] = np.array(train['Pclass'] == 2).astype(np.int32)train['P3'] = np.array(train['Pclass'] == 3).astype(np.int32)# testtest['P1'] = np.array(test['Pclass'] == 1).astype(np.int32)test['P2'] = np.array(test['Pclass'] == 2).astype(np.int32)test['P3'] = np.array(test['Pclass'] == 3).astype(np.int32)print(train.head(1)) PassengerId Survived Pclass Name Sex Age SibSp \ 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 Parch Ticket Fare Cabin Embarked P1 P2 P3 0 0 A/5 21171 7.25 NaN S 0 0 1 2. Sex性别：男or女 Sex label male 1 female 0 1234# 把male/female转换成1/0train['Sex'] = [1 if i == 'male' else 0 for i in train.Sex]test['Sex'] = [1 if i == 'male' else 0 for i in test.Sex]print(train.head(1)) PassengerId Survived Pclass Name Sex Age SibSp \ 0 1 0 3 Braund, Mr. Owen Harris 1 22.0 1 Parch Ticket Fare Cabin Embarked P1 P2 P3 0 0 A/5 21171 7.25 NaN S 0 0 1 3. SibSp and Parch SibSp the number of siblings/spouse：兄弟姐妹/配偶人数 Parch the number of children/parents：子女/父母人数 1234# 'FamilySize'：家庭成员人数for dataset in full_data: dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1print(train.head(1)) PassengerId Survived Pclass Name Sex Age SibSp \ 0 1 0 3 Braund, Mr. Owen Harris 1 22.0 1 Parch Ticket Fare Cabin Embarked P1 P2 P3 FamilySize 0 0 A/5 21171 7.25 NaN S 0 0 1 2 12345# 'IsAlone'：是否只身一人for dataset in full_data: dataset['IsAlone'] = 0 dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1print(train.head(1)) PassengerId Survived Pclass Name Sex Age SibSp \ 0 1 0 3 Braund, Mr. Owen Harris 1 22.0 1 Parch Ticket Fare Cabin Embarked P1 P2 P3 FamilySize IsAlone 0 0 A/5 21171 7.25 NaN S 0 0 1 2 0 4. Embarked登船港口，有缺失值，先进行缺失值处理 C = Cherbourg, Q = Queenstown, S = Southampton 12345678910111213# 缺失值处理for dataset in full_data: dataset['Embarked'] = dataset['Embarked'].fillna('S')# One-hot编码# traintrain['E1'] = np.array(train['Embarked'] == 'S').astype(np.int32)train['E2'] = np.array(train['Embarked'] == 'C').astype(np.int32)train['E3'] = np.array(train['Embarked'] == 'Q').astype(np.int32)# testtest['E1'] = np.array(test['Embarked'] == 'S').astype(np.int32)test['E2'] = np.array(test['Embarked'] == 'C').astype(np.int32)test['E3'] = np.array(test['Embarked'] == 'Q').astype(np.int32)print(train.head(1)) PassengerId Survived Pclass Name Sex Age SibSp \ 0 1 0 3 Braund, Mr. Owen Harris 1 22.0 1 Parch Ticket Fare Cabin Embarked P1 P2 P3 FamilySize IsAlone E1 \ 0 0 A/5 21171 7.25 NaN S 0 0 1 2 0 1 E2 E3 0 0 0 5. Fare乘客票价 12345678910111213141516171819# traintrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)train['CategoricalFare'].cat.categories = [1, 2, 3, 4]# one-hot编码train['F1'] = np.array(train['CategoricalFare'] == 1).astype(np.int32)train['F2'] = np.array(train['CategoricalFare'] == 2).astype(np.int32)train['F3'] = np.array(train['CategoricalFare'] == 3).astype(np.int32)train['F4'] = np.array(train['CategoricalFare'] == 4).astype(np.int32)# testtest['CategoricalFare'] = pd.qcut(test['Fare'], 4)test['CategoricalFare'].cat.categories = [1, 2, 3, 4]# one-hot编码test['F1'] = np.array(test['CategoricalFare'] == 1).astype(np.int32)test['F2'] = np.array(test['CategoricalFare'] == 2).astype(np.int32)test['F3'] = np.array(test['CategoricalFare'] == 3).astype(np.int32)test['F4'] = np.array(test['CategoricalFare'] == 4).astype(np.int32)print(train.head(1)) PassengerId Survived Pclass Name Sex Age SibSp \ 0 1 0 3 Braund, Mr. Owen Harris 1 22.0 1 Parch Ticket Fare ... FamilySize IsAlone E1 E2 E3 CategoricalFare \ 0 0 A/5 21171 7.25 ... 2 0 1 0 0 1 F1 F2 F3 F4 0 1 0 0 0 [1 rows x 25 columns] 6. Age缺失值处理 12345678for dataset in full_data: age_avg = dataset['Age'].mean() age_std = dataset['Age'].std() age_null_count = dataset['Age'].isnull().sum() age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count) dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list dataset['Age'] = dataset['Age'].astype(int)print(train.head(1)) PassengerId Survived Pclass Name Sex Age SibSp \ 0 1 0 3 Braund, Mr. Owen Harris 1 22 1 Parch Ticket Fare ... FamilySize IsAlone E1 E2 E3 CategoricalFare \ 0 0 A/5 21171 7.25 ... 2 0 1 0 0 1 F1 F2 F3 F4 0 1 0 0 0 [1 rows x 25 columns] d:\program files\python36\lib\site-packages\ipykernel_launcher.py:6: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy 1234567891011121314151617# traintrain['CategoricalAge'] = pd.qcut(train['Age'], 5)train['CategoricalAge'].cat.categories = [1, 2, 3, 4, 5]train['A1'] = np.array(train['CategoricalAge'] == 1).astype(np.int32)train['A2'] = np.array(train['CategoricalAge'] == 2).astype(np.int32)train['A3'] = np.array(train['CategoricalAge'] == 3).astype(np.int32)train['A4'] = np.array(train['CategoricalAge'] == 4).astype(np.int32)train['A5'] = np.array(train['CategoricalAge'] == 5).astype(np.int32)# testtest['CategoricalAge'] = pd.qcut(test['Age'], 5)test['CategoricalAge'].cat.categories = [1, 2, 3, 4, 5]test['A1'] = np.array(test['CategoricalAge'] == 1).astype(np.int32)test['A2'] = np.array(test['CategoricalAge'] == 2).astype(np.int32)test['A3'] = np.array(test['CategoricalAge'] == 3).astype(np.int32)test['A4'] = np.array(test['CategoricalAge'] == 4).astype(np.int32)test['A5'] = np.array(test['CategoricalAge'] == 5).astype(np.int32)print(train.head(1)) PassengerId Survived Pclass Name Sex Age SibSp \ 0 1 0 3 Braund, Mr. Owen Harris 1 22 1 Parch Ticket Fare ... F1 F2 F3 F4 CategoricalAge A1 A2 A3 A4 \ 0 0 A/5 21171 7.25 ... 1 0 0 0 2 0 1 0 0 A5 0 0 [1 rows x 31 columns] 7. Name新增一列特征’Title’：头衔 1234567891011def get_title(name): title_search = re.search(' ([A-Za-z]+)\.', name) # If the title exists, extract and return it. if title_search: return title_search.group(1) return ""for dataset in full_data: dataset['Title'] = dataset['Name'].apply(get_title)print(pd.crosstab(train['Title'], train['Sex'])) Sex 0 1 Title Capt 0 1 Col 0 2 Countess 1 0 Don 0 1 Dr 1 6 Jonkheer 0 1 Lady 1 0 Major 0 2 Master 0 40 Miss 182 0 Mlle 2 0 Mme 1 0 Mr 0 517 Mrs 125 0 Ms 1 0 Rev 0 6 Sir 0 1 12345678for dataset in full_data: dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\ 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare') dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss') dataset['Title'] = dataset['Title'].replace('Ms', 'Miss') dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')print (train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()) Title Survived 0 Master 0.575000 1 Miss 0.702703 2 Mr 0.156673 3 Mrs 0.793651 4 Rare 0.347826 12345678910111213# traintrain['T1'] = np.array(train['Title'] == 'Master').astype(np.int32)train['T2'] = np.array(train['Title'] == 'Miss').astype(np.int32)train['T3'] = np.array(train['Title'] == 'Mr').astype(np.int32)train['T4'] = np.array(train['Title'] == 'Mrs').astype(np.int32)train['T5'] = np.array(train['Title'] == 'Rare').astype(np.int32)# testtest['T1'] = np.array(test['Title'] == 'Master').astype(np.int32)test['T2'] = np.array(test['Title'] == 'Miss').astype(np.int32)test['T3'] = np.array(test['Title'] == 'Mr').astype(np.int32)test['T4'] = np.array(test['Title'] == 'Mrs').astype(np.int32)test['T5'] = np.array(test['Title'] == 'Rare').astype(np.int32)print(train.head(1)) PassengerId Survived Pclass Name Sex Age SibSp \ 0 1 0 3 Braund, Mr. Owen Harris 1 22 1 Parch Ticket Fare ... A2 A3 A4 A5 Title T1 T2 T3 T4 T5 0 0 A/5 21171 7.25 ... 1 0 0 0 Mr 0 0 1 0 0 [1 rows x 37 columns] 数据清洗获得训练神经网络的数据：train_x，train_y_ 以及预测样本：test_x 1train.columns Index([&apos;PassengerId&apos;, &apos;Survived&apos;, &apos;Pclass&apos;, &apos;Name&apos;, &apos;Sex&apos;, &apos;Age&apos;, &apos;SibSp&apos;, &apos;Parch&apos;, &apos;Ticket&apos;, &apos;Fare&apos;, &apos;Cabin&apos;, &apos;Embarked&apos;, &apos;P1&apos;, &apos;P2&apos;, &apos;P3&apos;, &apos;FamilySize&apos;, &apos;IsAlone&apos;, &apos;E1&apos;, &apos;E2&apos;, &apos;E3&apos;, &apos;CategoricalFare&apos;, &apos;F1&apos;, &apos;F2&apos;, &apos;F3&apos;, &apos;F4&apos;, &apos;CategoricalAge&apos;, &apos;A1&apos;, &apos;A2&apos;, &apos;A3&apos;, &apos;A4&apos;, &apos;A5&apos;, &apos;Title&apos;, &apos;T1&apos;, &apos;T2&apos;, &apos;T3&apos;, &apos;T4&apos;, &apos;T5&apos;], dtype=&apos;object&apos;) 123456train_x = train[[ 'P1', 'P2', 'P3', 'Sex', 'IsAlone', 'E1', 'E2', 'E3', 'F1', 'F2', 'F3', 'F4', 'A1', 'A2', 'A3', 'A4', 'A5', 'T1', 'T2', 'T3', 'T4', 'T5']]print(train_x.head(1)) P1 P2 P3 Sex IsAlone E1 E2 E3 F1 F2 ... A1 A2 A3 A4 A5 T1 \ 0 0 0 1 1 0 1 0 0 1 0 ... 0 1 0 0 0 0 T2 T3 T4 T5 0 0 1 0 0 [1 rows x 22 columns] 12train_y_ = train[['Survived']]print(train_y_.head(1)) Survived 0 0 123456test_x = test[[ 'P1', 'P2', 'P3', 'Sex', 'IsAlone', 'E1', 'E2', 'E3', 'F1', 'F2', 'F3', 'F4', 'A1', 'A2', 'A3', 'A4', 'A5', 'T1', 'T2', 'T3', 'T4', 'T5']]print(test_x.head(1)) P1 P2 P3 Sex IsAlone E1 E2 E3 F1 F2 ... A1 A2 A3 A4 A5 T1 \ 0 0 0 1 1 1 0 0 1 1 0 ... 0 0 0 1 0 0 T2 T3 T4 T5 0 0 1 0 0 [1 rows x 22 columns]]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Titanic</tag>
        <tag>数据分析</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客：百度、谷歌收录]]></title>
    <url>%2F2018%2F05%2F27%2FHexo%E5%8D%9A%E5%AE%A2%EF%BC%9A%E7%99%BE%E5%BA%A6%E3%80%81%E8%B0%B7%E6%AD%8C%E6%94%B6%E5%BD%95%2F</url>
    <content type="text"><![CDATA[写博客总是希望有读者，如何让自己的博客被搜索引擎网站收录，可以被大家搜索到就十分重要了！ 谷歌收录很快，基本上第二天就收录了；等了很久，自己的博客终于被百度收录了！所以，记录一下百度、谷歌收录的过程… 绑定个性域名添加站点地图sitemap 打开Hexo博客的根目录，打开cmd，安装sitemap及baidusitemap 12npm install hexo-generator-sitemap --savenpm install hexo-generator-baidu-sitemap --save 打开站点配置文件_config.yml，添加 12345# 添加站点地图sitemapsitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xml 百度收录验证网站是否被百度收录打开百度搜索，搜一下 1site:blog.luoyanbin.cn 如果搜索不到就说明没有收录 验证网站 打开百度站点信息的站点管理，点击添加网站 输入网站 验证网站，这里使用的验证方式是文件验证 下载验证文件，文件名类似于baidu_verify_xxxxxx.html 打开验证文件，在第一行添加下面内容，hexo g -d 后点击完成验证 12layout: false--- 提交链接完成验证后，接着是把我们的网站提交给百度，主要介绍一下三种方式 主动推送 主动推送比较复杂，还好有好心的博主帮我们做一个百度主动推送的插件hexo-baidu-url-submit 自动推送 NexT主题集成了自动推送功能，打开主题配置文件_config.yml，找到baidu_push，设置为true 1baidu_push: true sitemap提交 点击sitemap提交，输入之前生成的baidusitemap.xml网址，比如我提交的网址是 1https://blog.luoyanbin.cn/baidusitemap.xml 点击提交即可 谷歌收录 登录Search Console 和百度收录类似，先验证网站，验证方式也是一致的 验证完后点击网址进入console，点击站点地图（这里使用的是Google Search Console BETA版），输入sitemap.xml，提交即可]]></content>
      <categories>
        <category>博客搭建</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo搭建博客时遇到的错误Error]]></title>
    <url>%2F2018%2F05%2F24%2FHexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E9%94%99%E8%AF%AFError%2F</url>
    <content type="text"><![CDATA[Error: Host key verification failed. 错误代码 12345Error: Host key verification failed.fatal: Could not read from remote repository.Please make sure you have the correct access rightsand the repository exists. 解决办法 Git Bash Here 1ssh git@git.coding.net]]></content>
      <categories>
        <category>博客搭建</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Error</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas.DataFrame基础]]></title>
    <url>%2F2018%2F05%2F24%2FPandas-DataFrame%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[DataFrame基本操作查看数据DataFrame.head(n=5) 返回n行数据，默认显示5行 12345678910111213141516171819from pypower.case118 import case118import pandas as pdppc = case118()branch = pd.DataFrame(ppc[&apos;branch&apos;])branch.head()------------- 0 1 2 3 4 5 6 7 8 9 10 \0 1.0 2.0 0.03030 0.09990 0.02540 9900.0 0.0 0.0 0.0 0.0 1.01 1.0 3.0 0.01290 0.04240 0.01082 9900.0 0.0 0.0 0.0 0.0 1.02 4.0 5.0 0.00176 0.00798 0.00210 9900.0 0.0 0.0 0.0 0.0 1.03 3.0 5.0 0.02410 0.10800 0.02840 9900.0 0.0 0.0 0.0 0.0 1.04 5.0 6.0 0.01190 0.05400 0.01426 9900.0 0.0 0.0 0.0 0.0 1.0 11 120 -360.0 360.01 -360.0 360.02 -360.0 360.03 -360.0 360.04 -360.0 360.0 删除行、列DataFrame.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors=’raise’) labels: 单个标签或者标签列表 axis: 0表示行，1表示列 123456789branch1 = branch.drop([i for i in range(5,13)], axis=1)branch1.head()-------------- 0 1 2 3 40 1.0 2.0 0.03030 0.09990 0.025401 1.0 3.0 0.01290 0.04240 0.010822 4.0 5.0 0.00176 0.00798 0.002103 3.0 5.0 0.02410 0.10800 0.028404 5.0 6.0 0.01190 0.05400 0.01426 改变某一列数据类型DataFrame.astype(dtype, copy=True, errors=’raise’, **kwargs) 12345678910branch1[[0]] = branch1[[0]].astype(int)branch1[[1]] = branch1[[1]].astype(int)branch1.head()-------------- 0 1 2 3 40 1 2 0.03030 0.09990 0.025401 1 3 0.01290 0.04240 0.010822 4 5 0.00176 0.00798 0.002103 3 5 0.02410 0.10800 0.028404 5 6 0.01190 0.05400 0.01426 修改列标签DataFrame.rename(mapper=None, index=None, columns=None, axis=None, copy=True, inplace=False, level=None) columns: 字典类型 inplace：boolean类型, 默认是false。true表示在原DataFrame上修改，false相反。 123456789101112131415161718branch1.rename( columns=&#123; 0: &apos;支路首段&apos;, 1: &apos;支路末端&apos;, 2: &apos;支路电阻&apos;, 3: &apos;支路电抗&apos;, 4: &apos;支路电纳&apos; &#125;, inplace=True)branch1.head()-------------- 支路首段 支路末端 支路电阻 支路电抗 支路电纳0 1 2 0.03030 0.09990 0.025401 1 3 0.01290 0.04240 0.010822 4 5 0.00176 0.00798 0.002103 3 5 0.02410 0.10800 0.028404 5 6 0.01190 0.05400 0.01426]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
        <tag>DataFrame</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Selenium+Chrome获取异步加载数据]]></title>
    <url>%2F2018%2F05%2F24%2FSelenium-Chrome%E8%8E%B7%E5%8F%96%E5%BC%82%E6%AD%A5%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[环境 Python==3.6 selenium==3.12.0 Chrome==66.0.3359.181 Chromedriver==v2.37 基本使用1234567from selenium import webdriverfrom selenium.webdriver.chrome.options import Optionschrome_options = Options()chrome_options.add_argument(&apos;--headless&apos;)driver = webdriver.Chrome(chrome_options=chrome_options)driver.get(&apos;http://www.xxxx.html&apos;)html = driver.page_source]]></content>
      <categories>
        <category>爬虫基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
        <tag>Selenium</tag>
        <tag>Chrome</tag>
        <tag>Chromedriver</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB数据库基础]]></title>
    <url>%2F2018%2F05%2F20%2FMongoDB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[安装MongoDB MongoDB下载地址 Windows平台MongoDB安装教程 创建数据目录，MongoDB的数据目录不会主动创建，需要手动创建。 我的MongoDB数据目录为：F:\data\db 使用MongoDB启动数据库 启动命令 1mongod --dbpath F:\data\db 启动成功 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566672018-05-20T21:13:49.628+0800 I CONTROL [initandlisten] MongoDB starting : pid=3860 port=27017 dbpath=F:\data\db 64-bit host=PC-20180415ADZM2018-05-20T21:13:49.629+0800 I CONTROL [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R22018-05-20T21:13:49.629+0800 I CONTROL [initandlisten] db version v3.6.42018-05-20T21:13:49.629+0800 I CONTROL [initandlisten] git version: d0181a711f7e7f39e60b5aeb1dc7097bf6ae58562018-05-20T21:13:49.629+0800 I CONTROL [initandlisten] OpenSSL version: OpenSSL 1.0.2o-fips 27 Mar 20182018-05-20T21:13:49.629+0800 I CONTROL [initandlisten] allocator: tcmalloc2018-05-20T21:13:49.629+0800 I CONTROL [initandlisten] modules: none2018-05-20T21:13:49.629+0800 I CONTROL [initandlisten] build environment:2018-05-20T21:13:49.629+0800 I CONTROL [initandlisten] distmod: 2008plus-ssl2018-05-20T21:13:49.629+0800 I CONTROL [initandlisten] distarch: x86_642018-05-20T21:13:49.629+0800 I CONTROL [initandlisten] target_arch: x86_642018-05-20T21:13:49.629+0800 I CONTROL [initandlisten] options: &#123; storage: &#123; dbPath: &quot;F:\data\db&quot; &#125; &#125;2018-05-20T21:13:49.630+0800 I - [initandlisten] Detected data files in F:\data\db created by the &apos;wiredTiger&apos; storage engine, so setting the active storage engine to &apos;wiredTiger&apos;.2018-05-20T21:13:49.631+0800 I STORAGE [initandlisten] wiredtiger_open config:create,cache_size=1478M,session_max=20000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),cache_cursors=false,log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),statistics_log=(wait=0),verbose=(recovery_progress),2018-05-20T21:13:49.953+0800 I STORAGE [initandlisten] WiredTiger message [1526822029:953125][3860:1997878144], txn-recover: Main recovery loop: starting at 2/62722018-05-20T21:13:50.180+0800 I STORAGE [initandlisten] WiredTiger message [1526822030:180138][3860:1997878144], txn-recover: Recovering log 2 through 32018-05-20T21:13:50.279+0800 I STORAGE [initandlisten] WiredTiger message [1526822030:279144][3860:1997878144], txn-recover: Recovering log 3 through 32018-05-20T21:13:50.384+0800 I STORAGE [initandlisten] WiredTiger message [1526822030:384150][3860:1997878144], txn-recover: Set global recovery timestamp: 02018-05-20T21:13:50.934+0800 I CONTROL [initandlisten]2018-05-20T21:13:50.935+0800 I CONTROL [initandlisten] ** WARNING: Access control is not enabled for the database.2018-05-20T21:13:50.936+0800 I CONTROL [initandlisten] ** Read and write access to data and configuration is unrestricted.2018-05-20T21:13:50.937+0800 I CONTROL [initandlisten]2018-05-20T21:13:50.938+0800 I CONTROL [initandlisten] ** WARNING: This serveris bound to localhost.2018-05-20T21:13:50.939+0800 I CONTROL [initandlisten] ** Remote systems will be unable to connect to this server.2018-05-20T21:13:50.941+0800 I CONTROL [initandlisten] ** Start the server with --bind_ip &lt;address&gt; to specify which IP2018-05-20T21:13:50.942+0800 I CONTROL [initandlisten] ** addresses it should serve responses from, or with --bind_ip_all to2018-05-20T21:13:50.943+0800 I CONTROL [initandlisten] ** bind to allinterfaces. If this behavior is desired, start the2018-05-20T21:13:50.944+0800 I CONTROL [initandlisten] ** server with--bind_ip 127.0.0.1 to disable this warning.2018-05-20T21:13:50.945+0800 I CONTROL [initandlisten]2018-05-20T21:13:50.946+0800 I CONTROL [initandlisten] Hotfix KB2731284 or later update is not installed, will zero-out data files.2018-05-20T21:13:50.947+0800 I CONTROL [initandlisten]2018-05-20T21:13:50.948+0800 I CONTROL [initandlisten] ** WARNING: The file system cache of this machine is configured to be greater than 40% of the total memory. This can lead to increased memory pressure and poor performance.2018-05-20T21:13:50.949+0800 I CONTROL [initandlisten] See http://dochub.mongodb.org/core/wt-windows-system-file-cache2018-05-20T21:13:50.950+0800 I CONTROL [initandlisten]2018-05-20T21:13:52.430+0800 I FTDC [initandlisten] Initializing full-time diagnostic data capture with directory &apos;F:/data/db/diagnostic.data&apos;2018-05-20T21:13:52.527+0800 I NETWORK [initandlisten] waiting for connectionson port 27017]]></content>
      <categories>
        <category>数据库基础</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Titanic获救预测]]></title>
    <url>%2F2018%2F05%2F12%2FTitanic%E8%8E%B7%E6%95%91%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[引言Titanic获救预测是学习数据分析、机器学习、Tensorflow库的入门手练小项目，在本篇博客记录一下自己的学习过程，本项目的GitHub地址为Predict-survival-on-the-Titanic 修改了参考文献的教程中的一些小错误 项目介绍Titanic沉船事故是历史上最著名的沉船事故之一。1912年4月15日，在它首次航行的过程中，Titanic号与冰山相撞后沉没，2224名乘客和机组人员中有1502人遇难。这场轰动一时的悲剧震惊了国际社会，随后产生了更好的船舶安全管理条例。 在本次项目中，需要分析哪一类人群更可能在沉船事故中生存下来。使用机器学习工具预测那些乘客在悲剧中存活下来。 代码介绍data_preprocess.py对数据进行预处理 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# coding: utf-8import pandas as pdimport osimport numpy as npdef train_preprocess(): # 读取train.csv为pandas.DataFrame train = pd.read_csv(os.getcwd() + &apos;\\data\\train.csv&apos;) # 数据预处理 train[&apos;Age&apos;] = train[&apos;Age&apos;].fillna(train[&apos;Age&apos;].mean()) train[&apos;Cabin&apos;] = pd.factorize(train.Cabin)[0] train.fillna(0, inplace=True) train[&apos;Sex&apos;] = [1 if i == &apos;male&apos; else 0 for i in train.Sex] # 处理Pclass train[&apos;P1&apos;] = np.array(train[&apos;Pclass&apos;] == 1).astype(np.int32) train[&apos;P2&apos;] = np.array(train[&apos;Pclass&apos;] == 2).astype(np.int32) train[&apos;P3&apos;] = np.array(train[&apos;Pclass&apos;] == 3).astype(np.int32) del train[&apos;Pclass&apos;] # 处理Embarked train[&apos;E1&apos;] = np.array(train[&apos;Embarked&apos;] == &apos;S&apos;).astype(np.int32) train[&apos;E2&apos;] = np.array(train[&apos;Embarked&apos;] == &apos;C&apos;).astype(np.int32) train[&apos;E3&apos;] = np.array(train[&apos;Embarked&apos;] == &apos;Q&apos;).astype(np.int32) del train[&apos;Embarked&apos;] # 得到train_x, train_y_ train_x = train[[ &apos;Sex&apos;, &apos;Age&apos;, &apos;SibSp&apos;, &apos;Parch&apos;, &apos;Fare&apos;, &apos;Cabin&apos;, &apos;P1&apos;, &apos;P2&apos;, &apos;P3&apos;, &apos;E1&apos;, &apos;E2&apos;, &apos;E3&apos; ]] train_y_ = train[&apos;Survived&apos;].values.reshape(len(train), 1) return train_x, train_y_def test_preproces(): # 读取test.csv为pandas.DataFrame test = pd.read_csv(os.getcwd() + &apos;\\data\\test.csv&apos;) # 数据预处理 test[&apos;Age&apos;] = test[&apos;Age&apos;].fillna(test[&apos;Age&apos;].mean()) test[&apos;Cabin&apos;] = pd.factorize(test.Cabin)[0] test.fillna(0, inplace=True) test[&apos;Sex&apos;] = [1 if i == &apos;male&apos; else 0 for i in test.Sex] # 处理Pclass test[&apos;P1&apos;] = np.array(test[&apos;Pclass&apos;] == 1).astype(np.int32) test[&apos;P2&apos;] = np.array(test[&apos;Pclass&apos;] == 2).astype(np.int32) test[&apos;P3&apos;] = np.array(test[&apos;Pclass&apos;] == 3).astype(np.int32) del test[&apos;Pclass&apos;] # 处理Embarked test[&apos;E1&apos;] = np.array(test[&apos;Embarked&apos;] == &apos;S&apos;).astype(np.int32) test[&apos;E2&apos;] = np.array(test[&apos;Embarked&apos;] == &apos;C&apos;).astype(np.int32) test[&apos;E3&apos;] = np.array(test[&apos;Embarked&apos;] == &apos;Q&apos;).astype(np.int32) del test[&apos;Embarked&apos;] # 得到test_x test_x = test[[ &apos;Sex&apos;, &apos;Age&apos;, &apos;SibSp&apos;, &apos;Parch&apos;, &apos;Fare&apos;, &apos;Cabin&apos;, &apos;P1&apos;, &apos;P2&apos;, &apos;P3&apos;, &apos;E1&apos;, &apos;E2&apos;, &apos;E3&apos; ]] return test_x forward.py前向传播 1234567891011121314151617181920import tensorflow as tfdef get_weight(shape): w = tf.Variable(tf.random_normal(shape)) return wdef get_bias(shape): b = tf.Variable(tf.random_normal(shape)) return bdef forward(x): w = get_weight([12, 1]) b = get_bias([1]) y = tf.matmul(x, w) + b pred = tf.cast(tf.sigmoid(y) &gt; 0.5, tf.float32) return y, pred backward.py反向传播 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import tensorflow as tfimport numpy as npfrom data_preprocess import train_preprocessfrom forward import forwardSTEPS = 25000BATCH_SIZE = 100def backward(): x = tf.placeholder(tf.float32, shape=[None, 12]) y_ = tf.placeholder(tf.float32, shape=[None, 1]) train_x, train_y_ = train_preprocess() y, pred = forward(x) # 定义损失函数loss loss = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=y)) # 定义反向传播方法 train_step = tf.train.GradientDescentOptimizer(0.0003).minimize(loss) # 定义准确率 accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y_), tf.float32)) with tf.Session() as sess: init_op = tf.global_variables_initializer() sess.run(init_op) train_loss = [] train_acc = [] for i in range(STEPS): index = np.random.permutation(len(train_y_)) train_x = train_x.take(index) train_y_ = train_y_[index] for j in range(len(train_y_) // 100 + 1): start = j * BATCH_SIZE end = start + BATCH_SIZE sess.run( train_step, feed_dict=&#123; x: train_x[start:end], y_: train_y_[start:end] &#125;) if i % 1000 == 0: train_loss_temp = sess.run( loss, feed_dict=&#123; x: train_x[start:end], y_: train_y_[start:end] &#125;) train_loss.append(train_loss_temp) train_acc_temp = sess.run( accuracy, feed_dict=&#123; x: train_x[start:end], y_: train_y_[start:end] &#125;) train_acc.append(train_acc_temp) print(train_loss_temp, &apos; &apos;, train_acc_temp)if __name__ == &apos;__main__&apos;: backward() 运行结果1234567891011121314151617181920212223242525.031631 0.28571430.6623061 0.70329670.6302701 0.68131870.6674858 0.65934070.49119952 0.725274740.49594036 0.78021980.44275057 0.846153860.44745535 0.79120880.44932628 0.81318680.5412523 0.736263750.48308572 0.79120880.4907554 0.78021980.41890615 0.86813190.4681515 0.79120880.44097015 0.857142870.45140973 0.78021980.39417002 0.857142870.43789506 0.835164840.4205189 0.81318680.45825815 0.80219780.4797148 0.80219780.4898475 0.79120880.46390718 0.79120880.37100115 0.846153860.41760778 0.8021978 未完待续对测试集数据进行预测后，在Kaggle上Submit Prediction的成绩为0.76555 如果有提高准确率的方法，希望可以在评论去留言，大家一起讨论！ 参考文献 Tensorflow基础之泰坦尼克获救预测]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Titanic</tag>
        <tag>数据分析</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装Python爬虫框架：Scrapy]]></title>
    <url>%2F2018%2F05%2F09%2F%E5%AE%89%E8%A3%85Python%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%EF%BC%9AScrapy%2F</url>
    <content type="text"><![CDATA[想要用Python进行数据分析，首先你得有数据呀！利用Scrapy爬虫框架爬取数据十分方便，那么，首先安装一下Scrapy框架吧！ 如果希望安装过程比较简单的话，推荐使用Anaconda，利用Anaconda的包管理工具conda可以方便快捷的安装Scrapy！ 我使用的Python开发环境是Python3.6+VSCode，记录一下我的安装过程吧！ pip3安装Scrapy1pip3 install Scrapy 然后报错了！ 12345678Command &quot;c:\users\administrator\envs\base\scripts\python.exe -u -c &quot;import setuptools, tokenize;__file__=&apos;C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\pip-install-vnn_943_\\Twisted\\setup.py&apos;;f=getattr(tokenize, &apos;open&apos;, open)(__file__);code=f.read().replace(&apos;\r\n&apos;, &apos;\n&apos;);f.close();exec(compile(code, __file__, &apos;exec&apos;))&quot; install --record C:\Users\ADMINI~1\AppData\Local\Temp\pip-record-3gc47464\install-record.txt --single-version-externally-managed --compile --install-headers c:\users\administrator\envs\base\include\site\python3.6\Twisted&quot; failed with error code 1 in C:\Users\ADMINI~1\AppData\Local\Temp\pip-install-vnn_943_\Twisted\ 查看了参考文献，发现twisted库安装失败！ 安装twisted库twisted库不能通过pip安装 点击这里，下载相应版本的twisted，我下载的是Twisted-18.4.0-cp36-cp36m-win_amd64.whl 打开cmd，cd到Twisted-18.4.0-cp36-cp36m-win_amd64.whl的所在目录 安装Twisted-18.4.0-cp36-cp36m-win_amd64.whl 1pip3 install Twisted-18.4.0-cp36-cp36m-win_amd64.whl 查看是否安装成功 12345678910111213141516171819202122232425262728293031pip3 list---------Package Version-------------- -------asn1crypto 0.24.0attrs 18.1.0Automat 0.6.0cffi 1.11.5constantly 15.1.0cryptography 2.2.2cssselect 1.0.3flake8 3.5.0hyperlink 18.0.0idna 2.6incremental 17.5.0lxml 4.2.1mccabe 0.6.1parsel 1.4.0Pillow 5.1.0pip 10.0.1pycodestyle 2.3.1pycparser 2.18PyDispatcher 2.0.5pyflakes 1.6.0pyOpenSSL 17.5.0setuptools 39.1.0six 1.11.0Twisted 18.4.0w3lib 1.19.0wheel 0.31.0zope.interface 4.5.0 测试Scrapy是否安装成功打开cmd，输入scrapy -h 12345678910111213141516171819Scrapy 1.5.0 - no active projectUsage: scrapy &lt;command&gt; [options] [args]Available commands: bench Run quick benchmark test fetch Fetch a URL using the Scrapy downloader genspider Generate new spider using pre-defined templates runspider Run a self-contained spider (without creating a project) settings Get settings values shell Interactive scraping console startproject Create new project version Print Scrapy version view Open URL in browser, as seen by Scrapy [ more ] More commands available when run from project directoryUse &quot;scrapy &lt;command&gt; -h&quot; to see more info about a command 安装pywin32运行的时候报错 1exceptions.ImportError: No module named win32api 点击这里，下载相应版本的pywin32，我下载的是pywin32-223-cp36-cp36m-win_amd64.whl 打开cmd，cd到pywin32-223-cp36-cp36m-win_amd64.whl的所在目录 安装pywin32-223-cp36-cp36m-win_amd64.whl 找到pywin32库，路径为..\Lib\site-packages\pywin32_system32 复制该文件夹下的两个文件： pythoncom36.dll pywintypes36.dll 复制到C:\Windows\System32路径下 ok完成，测试一下是否安装成功 12# 打开cmd，输入python，回车import win32api 没有报错，即为安装成功！ 参考文献 从零开始学Python网络爬虫[M]. 罗攀, 蒋仟.]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[写Python时遇到的警告Warning和错误Error]]></title>
    <url>%2F2018%2F05%2F08%2F%E5%86%99Python%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E8%AD%A6%E5%91%8Awarning%E5%92%8C%E9%94%99%E8%AF%AFerror%2F</url>
    <content type="text"><![CDATA[警告Warningnumbanumba是由Anaconda发起的一款支持NumPy开源的Python优化编译器。它使用卓越的LLVM编译器基础结构将Python语法编译为机器代码。 错误代码 1numba cannot be imported and numba functions are disabled. 解决方法 安装numba 1pip3 install numba 错误Error写Tensorflow遇到的错误InvalidArgumentError 问题一描述 12InvalidArgumentError: You must feed a value for placeholder tensor &apos;Placeholder_1&apos; with dtype float and shape [?,1][[Node: Placeholder_1 = Placeholder[dtype=DT_FLOAT, shape=[?,1], _device=&quot;/job:localhost/replica:0/task:0/device:CPU:0&quot;]()]] 问题一解决办法 参考这两篇文章[1]、[2] 写Pandas遇到的错误ValueError 问题一描述 1234567Code:-----data = data[(data[&apos;tournament&apos;] == &apos;FIFA World Cup&apos;) or (data[&apos;tournament&apos;] == &apos;FIFA World Cup qualification&apos;)]Error:------ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). 问题一解决办法 1234567Method:-------use &quot;bitwise&quot; | instead of &quot;or&quot;Modify:-------data = data[(data[&apos;tournament&apos;] == &apos;FIFA World Cup&apos;) | (data[&apos;tournament&apos;] == &apos;FIFA World Cup qualification&apos;)] 问题二描述 1234567Code:-----full[full[&apos;Coupon_id&apos;] != &apos;null&apos;] = full[full[&apos;Coupon_id&apos;] != &apos;null&apos;].astype(np.int)Error:------ValueError: invalid literal for int() with base 10: &apos;11002.0&apos; 问题二解决办法 1234567Method:-------异常出现的原因是使用int()方法直接对浮点数字符（如：&apos;11002.0&apos;）进行强制转换；int()方法要求字符类型只能是整数，而不能是浮点数Modify:-------int(float(&apos;11002.0&apos;))]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>Error</tag>
        <tag>Python</tag>
        <tag>Warning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用VSCode写Python的个人配置及配置同步]]></title>
    <url>%2F2018%2F05%2F07%2F%E7%94%A8VSCode%E5%86%99Python%E7%9A%84%E4%B8%AA%E4%BA%BA%E9%85%8D%E7%BD%AE%E5%8F%8A%E9%85%8D%E7%BD%AE%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[VSCode配置flake8配置Flake8 是由Python官方发布的一款辅助检测Python代码是否规范的工具，相对于目前热度比较高的Pylint来说，Flake8检查规则灵活，支持集成额外插件，扩展性强。 flake8安装1pip install flake8 flake8个人配置1234&#123; &quot;python.linting.flake8Enabled&quot;: true, &quot;python.linting.flake8Args&quot;: [&quot;--max-line-length=120&quot;,&quot;--ignore=W292&quot;]&#125; code sample message W292 no newline at end of file yapf配置YAPF采用了不同的方法，基于Daniel Jasper开发的“’clang-format”。从本质上来说，该算法取走代码并重新排版，以符合样式指南的最佳格式，即便原始代码没有违反样式指南。 yapf安装1pip install yapf yapf使用 在代码编辑区右键，选择格式化文件 键盘快捷键：Alt+Shift+F 配置同步 在VSCode上安装Settings Sync插件 登录GitHub，点击Settings，找到Developer Settings的Personal access tokens，点击Generate new token - 在Token Description中输入Token名称，例如：VSCode Settings Sync - 在下面的选项中选择gist - 保存Token 上传/更新设置 打开VSCode命令面板，输入sync: Update/Upload Settings 输入刚才保存的Token 上传/更新设置后，会有一个CODE SETTINGS SYNC UPLOAD SUMMARY，将这个输出保存为Settings-sync-summary.txt文件，里面有Gist和Token， Settings-sync-summary.txt文件如下12345678910111213141516171819202122CODE SETTINGS SYNC UPLOAD SUMMARYVersion: 2.9.2--------------------GitHub Token: your tokenGitHub Gist: your gistGitHub Gist Type: SecretRestarting Visual Studio Code may be required to apply color and file icon theme.--------------------Files Uploaded: Extensions Removed:Extensions Added: code-settings-sync v2.9.2 gc-excelviewer v2.0.21 markdown-all-in-one v1.3.0 python v2018.4.0 vscode-icons v7.23.0 vscode-language-pack-zh-hans v1.23.5--------------------Done. 下载配置 在新电脑上VSCode安装好Settings Sync插件后，打开命令面板输入sync: Download Settings 在命令面板输入GitHub Token及Gist ID 耐心等待同步完成…]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>VSCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python虚拟环境]]></title>
    <url>%2F2018%2F05%2F06%2FPython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[安装虚拟环境12pip install virtualenvpip install virtualenvwrapper-win virtualenvwrapper命令 命令 说明 mkvirtualenv venv 创建一个名为venv虚拟环境 lsvirtualenv 列出所有虚拟环境 workon venv 激活虚拟环境 cdvirtualenv 进入虚拟环境目录 cdsitepackages 进入虚拟环境的site-packages目录 lssitepackages 列出site-packages目录的所有软件包 deactivate 停止虚拟环境 rmvitualenv venv 删除虚拟环境 pip freeze &gt;packages.txt 冻结(freeze) 环境 pip install -r packages.txt 重建(rebuild)]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>virtualenv</tag>
        <tag>virtualenvwrapper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python更换pip源至国内源]]></title>
    <url>%2F2018%2F05%2F06%2FPython%E6%9B%B4%E6%8D%A2pip%E6%BA%90%E8%87%B3%E5%9B%BD%E5%86%85%E6%BA%90%2F</url>
    <content type="text"><![CDATA[将Python的pip源更换成国内的安装源，可以提高Python库的安装速度！ Ubuntu 16.04在 /home/yourUserName/ 目录下新建一个 .pip 文件夹，进入改文件夹，新建一个 pip.conf 文件 打开文件，输入以下内容： 1234[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple[install]trusted-host = pypi.tuna.tsinghua.edu.cn Windows7系统 找到C盘Users文件下应用程序文件存放目录，新建一个pip文件夹； 1234# 公用账户路径C:\Users\Administrator# 个人账户路径C:\Users\AccountName\AppData\Local 在pip文件夹下新建一个pip.ini文件，文件内容为： 更换为豆瓣源 1234[global]index-url = http://pypi.douban.com/simple[install]trusted-host = pypi.douban.com 更换为清华源 1234[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple[install]trusted-host = pypi.tuna.tsinghua.edu.cn]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>pip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas库学习笔记]]></title>
    <url>%2F2018%2F05%2F05%2FPandas%E5%BA%93%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Pandas库入门Pandas库的介绍 Pandas是Python第三方库，提供高性能易用数据类型和分析工具 Pandas基于NumPy实现，常与NumPy和Matplotlib一同使用 Pandas库的引用1import pandas as pd Pandas库的理解 两个数据类型：Series, DataFrame 基于上述数据类型的各类操作——基本操作、运算操作、特征类操作、关联类操作 NumPy Pandas 基础数据类型 扩展数据类型 关注数据的结构表达 关注数据的应用表达 维度：数据间关系 数据与索引间关系 Pandas库的Series类型Series类型Series类型由一组数据及与之相关的数据索引组成 Series是一维带“标签”数组 1index_0 -&gt; data_a Series基本操作类似ndarray和字典，根据索引对齐 Series类型可以由如下类型创建： Python列表，index与列表元素个数一致 标量值，index表达Series类型的尺寸 Python字典，键值对中的“键”是索引，index从字典中进行选择操作 ndarray，索引和数据都可以通过ndarray类型创建 其他函数，range()函数等 Series类型的基本操作Series类型包括index和values两部分 函数 说明 .index 获得索引 .values 获得数据 自动索引和自定义索引并存 两套索引并存，但不能混用 Series类型的操作类似ndarray类型 索引方法相同，采用[] NumPy中运算和操作可用于Series类型 可以通过自定义索引的列表进行切片 可以通过自动索引进行切片，如果存在自定义索引，则一同被切片 Series类型的操作类似Python字典类型 通过自定义索引访问 保留字in操作 使用.get()方法 Series类型对齐操作Series类型在运算中会自动对齐不同索引的数据 Series类型的name属性Series对象和索引都可以有一个名字，存储在属性.name中 Series类型的修改Series对象可以随时修改并即刻生效 Series类型的常用函数 方法 说明 .fillna(**kwargs) Fill NA/NaN values using the specified method Pandas库的DataFrame类型DataFrame类型DataFrame类型由共用相同索引的一组列组成 1234index_0 -&gt; data_a data_1 ... data_windex_1 -&gt; data_b data_2 ... data_xindex_2 -&gt; data_c data_3 ... data_yindex_3 -&gt; data_d data_4 ... data_z DataFrame是一个表格型的数据类型，每列值类型可以不同 DataFrame既有行索引、也有列索引 DataFrame常用于表达二维数据，但可以表达多维数据 DataFrame类型可以由如下类型创建： 二维ndarray对象 由一维ndarray、列表、字典、元组或Series构成的字典 Series类型 其他的DataFrame类型 DataFrame是二维带“标签”数组 DataFrame基本操作类似Series，依据行列索引 Pandas库的数据类型操作重新索引.reindex()能够改变或重排Series和DataFrame索引 .reindex(index=None, columns=None, …)的参数 参数 说明 index, columns 新的行列自定义索引 fill_value 重新索引中，用于填充缺失位置的值 method 填充方法, ffill当前值向前填充，bfill向后填充 limit 最大填充量 copy 默认True，生成新的对象，False时，新旧相等不复制 Series和DataFrame的索引是Index类型 Index对象是不可修改类型 索引类型的常用方法 方法 说明 .append(idx) 连接另一个Index对象，产生新的Index对象 .diff(idx) 计算差集，产生新的Index对象 .intersection(idx) 计算交集 .union(idx) 计算并集 .delete(loc) 删除loc位置处的元素 .insert(loc,e) 在loc位置增加一个元素e .drop 能够删除Series和DataFrame指定行或列索引 Pandas库的数据类型运算算术运算法则 算术运算根据行列索引，补齐后运算，运算默认产生浮点数 补齐时缺项填充NaN (空值) 二维和一维、一维和零维间为广播运算 采用+ ‐ * /符号进行的二元运算产生新的对象 数据类型的算术运算 方法 说明 .add(d, **argws) 类型间加法运算，可选参数 .sub(d, **argws) 类型间减法运算，可选参数 .mul(d, **argws) 类型间乘法运算，可选参数 .div(d, **argws) 类型间除法运算，可选参数 fill_value参数替代NaN，替代后参与运算 不同维度间为广播运算，一维Series默认在轴1参与运算 使用运算方法可以令一维Series参与轴0运算 数据类型的比较运算 同维度运算，尺寸一致 不同维度，广播运算，默认在1轴 Pandas常用函数 方法 说明 .factorize(values, sort=False, order=None, na_sentinel=-1, size_hint=None) Encode input values as an enumerated type or categorical variable 参考文献 Python数据分析与展示-北京理工大学-嵩天]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
        <tag>Series</tag>
        <tag>DataFrame</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy库学习笔记]]></title>
    <url>%2F2018%2F05%2F01%2FNumpy%E5%BA%93%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Numpy库入门数据的维度维度是指一组数据的组织形式 一维数据一维数据由对等关系的有序或无序数据构成，采用线性方式组织，对应列表、数组和集合等概念。 二维数据二维数据由多个一维数据构成，是一维数据的组合形式 多维数据多维数据由一维或二维数据在新维度上扩展形成 高维数据高维数据仅利用最基本的二元关系展示数据间的复杂结构 数据维度的Python表示 数据维度 Python表示 一维数据 列表和集合 二维数据 列表 多维数据 列表 高维数据 字典或数据表示格式JSON、XML和YAML格式 Numpy的数组对象：ndarrayNumpy库介绍Numpy是一个开源的Python科学计算基础库，包含 一个强大的N维数组对象 ndarray 广播功能函数 整合C/C++/Fortran代码的工具 线性代数、傅里叶变换、随机数生成等功能 Numpy是Scipy、Pandas等数据处理或科学计算库的基础 Numpy的引用1import numpy as np N维数组对象：ndarrayndarray和列表的区别 数组对象可以去掉元素间运算所需的循环，使一维向量更像单个数据 设置专门的数组对象，经过优化，可以提升这类应用的运算速度观察：科学计算中，一个维度所有数据的类型往往相同 数组对象采用相同的数据类型，有助于节省运算和存储空间 ndarray对象ndarray是一个多维数组对象，由两部分构成： 实际的数据 描述这些数据的元数据（数据维度、数据类型等） ndarray数组一般要求所有元素类型相同（同质），数组下标从0开始 ndarray对象的属性 属性 说明 .ndim 秩，即轴的数量或维度的数量 .shape ndarray对象的尺度，对于矩阵，n行m列 .size ndarray对象元素的个数，相当于.shape中n*m的值 .dtype ndarray对象的元素类型 .itemsize ndarray对象中每个元素的大小，以字节为单位 ndarray对象的元素类型 数据类型 说明 bool 布尔类型，True或False intc 与C语言中的int类型一致，一般是int32或int64 intp 用于索引的整数，与C语言中ssize_t一致，int32或int64 int8 字节长度的整数，取值：[‐128, 127] int16 16位长度的整数，取值：[‐32768, 32767] int32 32位长度的整数，取值：[‐$2^{31}$,$2^{31}$‐1] int64 64位长度的整数，取值：[‐$2^{63}$,$2^{63}$‐1] uint8 8位无符号整数，取值：[0, 255] uint16 16位无符号整数，取值：[0, 65535] uint32 32位无符号整数，取值：[0,$2^{31}$‐1] uint64 32位无符号整数，取值：[0,$2^{63}$‐1] float16 16位半精度浮点数：1位符号位，5位指数，10位尾数 float32 32位半精度浮点数：1位符号位，8位指数，23位尾数 float64 64位半精度浮点数：1位符号位，11位指数，52位尾数 complex64 复数类型，实部和虚部都是32位浮点数 complex128 复数类型，实部和虚部都是64位浮点数 Python语法仅支持整数、浮点数和复数3种类型ndarray为什么要支持这么多种元素类型？ 科学计算涉及数据较多，对存储和性能都有较高要求 对元素类型精细定义，有助于NumPy合理使用存储空间并优化性能 对元素类型精细定义，有助于程序员对程序规模有合理评估 非同质的ndarray对象使用较少，不作讨论！ ndarray数组的创建方法 从Python中的列表、元组等类型创建ndarray数组 12x = np.array(list/tuple)x = np.array(list/tuple, dtype=np.float32) 当np.array()不指定dtype时，NumPy将根据数据情况关联一个dtype类型 使用NumPy中函数创建ndarray数组，如：arange, ones, zeros等 函数 说明 np.arange(n) 类似range()函数，返回ndarray类型，元素从0到n‐1 np.ones(shape) 根据shape生成一个全1数组，shape是元组类型 np.zeros(shape) 根据shape生成一个全0数组，shape是元组类型 np.full(shape,val) 根据shape生成一个数组，每个元素值都是val np.eye(n) 创建一个正方的n*n单位矩阵，对角线为1，其余为0 np.ones_like(a) 根据数组a的形状生成一个全1数组 np.zeros_like(a) 根据数组a的形状生成一个全0数组 np.full_like(a,val) 根据数组a的形状生成一个数组，每个元素值都是val ndarray数组的变换对于创建后的ndarray数组，可以对其进行维度变换和元素类型变换 方法 说明 .reshape(shape) 不改变数组元素，返回一个shape形状的数组，原数组不变 .resize(shape) 与.reshape()功能一致，但修改原数组 .swapaxes(ax1,ax2) 将数组n个维度中两个维度进行调换 .flatten() 对数组进行降维，返回折叠后的一维数组，原数组不变 .astype(new_type) astype()方法一定会创建新的数组（原始数据的一个拷贝），即使两个类型一致，new_type是新创建数组的数据类型，例如，np.float .tolist() ndarray数组向列表的转换 ndarray数组的操作数组的索引和切片 一维数组的索引和切片：与Python的列表类似 123# 编号0开始从左递增，或‐1开始从右递减a[2] #第3个元素a[1:4:2] #起始编号: 终止编号(不含): 步长，3元素冒号分割 多维数组的索引： 123# 每个维度一个索引值，逗号分割a[0,1,2]a[-1,-2,-3] 多维数组的切片： 1234# 选取一个维度用:a[:,1,-3] #选取了第一个维度a[:,1:3,:] #每个维度切片方法与一维数组相同a[:,:,::2] #每个维度可以使用步长跳跃切片 ndarray数组的运算 数组与标量之间的运算作用于数组的每一个元素 Numpy一元函数 对ndarray中的数据执行元素级运算的函数 函数 说明 np.abs(x) np.fabs(x) 计算数组各元素的绝对值 np.sqrt(x) 计算数组各元素的平方根 np.square(x) 计算数组各元素的平方 np.log(x) np.log10(x) np.log2(x) 计算数组各元素的自然对数、10底对数和2底对数 np.ceil(x) np.floor(x) 计算数组各元素的ceiling值 或 floor值 np.rint(x) 计算数组各元素的四舍五入值 np.modf(x) 将数组各元素的小数和整数部分以两个独立数组形式返回 np.cos(x) np.cosh(x) np.sin(x) np.sinh(x) np.tan(x) np.tanh(x) 计算数组各元素的普通型和双曲型三角函数 np.exp(x) 计算数组各元素的指数值 np.sign(x) 计算数组各元素的符号值，1(+), 0, ‐1(‐) Numpy二元函数 函数 说明 + ‐ * / ** 两个数组各元素进行对应运算 np.maximum(x,y) np.fmax() np.minimum(x,y) np.fmin() 元素级的最大值/最小值计算 np.mod(x,y) 元素级的模运算 np.copysign(x,y) 将数组y中各元素值的符号赋值给数组x对应元素 &gt; &lt; &gt;= &lt;= == != 算术比较，产生布尔型数组 Numpy数据存储与读取.csv文件的存取CSV(Comma‐Separated Value, 逗号分隔值)是一种常见的文件格式，用来存储批量数据 .csv文件的存储1np.savetxt(frame, array, fmt=&apos;%.18e&apos;, delimiter=None) frame : 文件、字符串或产生器，可以是.gz或.bz2的压缩文件 array : 存入文件的数组 fmt : 写入文件的格式，例如：%d %.2f %.18e delimiter : 分割字符串，默认是任何空格 .csv文件的读取1np.loadtxt(frame, dtype=np.float, delimiter=None，unpack=False) frame : 文件、字符串或产生器，可以是.gz或.bz2的压缩文件 dtype : 数据类型，可选 delimiter : 分割字符串，默认是任何空格 unpack : 如果True，读入属性将分别写入不同变量 .csv文件的局限性 CSV只能有效存储一维和二维数组 np.savetxt() np.loadtxt()只能有效存取一维和二维数组 多维数据的存取多维数据的存储1a.tofile(frame, sep=&apos;&apos;, format=&apos;%s&apos;) frame : 文件、字符串 sep : 数据分割字符串，如果是空串，写入文件为二进制 format : 写入数据的格式 多维数据的读取1np.fromfile(frame, dtype=float, count=‐1, sep=&apos;&apos;) frame : 文件、字符串 dtype : 读取的数据类型 count : 读入元素个数，‐1表示读入整个文件 sep : 数据分割字符串，如果是空串，写入文件为二进制 注意该方法需要读取时知道存入文件时数组的维度和元素类型，a.tofile()和np.fromfile()需要配合使用，可以通过元数据文件来存储额外信息 Numpy的便捷文件存取Numpy的便捷文件存储12np.save(fname, array)np.savez(fname, array) fname : 文件名，以.npy为扩展名，压缩扩展名为.npz array : 数组变量 Numpy的便捷文件读取1np.load(fname) fname : 文件名，以.npy为扩展名，压缩扩展名为.npz Numpy的随机数函数Numpy的随机数函数子库NumPy的random子库1np.random np.random的随机数函数（一） 函数 说明 rand(d0,d1,..,dn) 根据d0‐dn创建随机数数组，浮点数，[0,1)，均匀分布 randn(d0,d1,..,dn) 根据d0‐dn创建随机数数组，标准正态分布 randint(low[,high,shape]) 根据shape创建随机整数或整数数组，范围是[low, high) seed(s) 随机数种子，s是给定的种子值 np.random的随机数函数（二） 函数 说明 shuffle(a) 根据数组a的第1轴进行随排列，改变数组x permutation(a) 根据数组a的第1轴产生一个新的乱序数组，不改变数组x choice(a[,size,replace,p]) 从一维数组a中以概率p抽取元素，形成size形状新数组 replace表示是否可以重用元素，默认为False np.random的随机数函数（三） 函数 说明 uniform(low,high,size) 产生具有均匀分布的数组,low起始值,high结束值,size形状 normal(loc,scale,size) 产生具有正态分布的数组,loc均值,scale标准差,size形状 poisson(lam,size) 产生具有泊松分布的数组,lam随机事件发生率,size形状 Numpy的统计函数Numpy直接提供的统计类函数 123np.std()np.var() np.average() Numpy的统计函数（一） 函数 说明 sum(a, axis=None) 根据给定轴axis计算数组a相关元素之和，axis整数或元组 mean(a, axis=None) 根据给定轴axis计算数组a相关元素的期望，axis整数或元组 average(a,axis=None,weights=None) 根据给定轴axis计算数组a相关元素的加权平均值 std(a, axis=None) 根据给定轴axis计算数组a相关元素的标准差 var(a, axis=None) 根据给定轴axis计算数组a相关元素的方差 [注]axis=None 是统计函数的标配参数 Numpy的统计函数（二） 函数 说明 min(a) max(a) 计算数组a中元素的最小值、最大值 argmin(a) argmax(a) 计算数组a中元素最小值、最大值的降一维后下标 unravel_index(index, shape) 根据shape将一维下标index转换成多维下标 ptp(a) 计算数组a中元素最大值与最小值的差 median(a) 计算数组a中元素的中位数（中值） Numpy的梯度函数 函数 说明 np.gradient(f) 计算数组f中元素的梯度，当f为多维时，返回每个维度梯度 梯度：连续值之间的变化率，即斜率 XY坐标轴连续三个X坐标对应的Y轴值：a, b, c，其中，b的梯度是： (c‐a)/2 参考文献 Python数据分析与展示-北京理工大学-嵩天]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客主题: NexT]]></title>
    <url>%2F2018%2F04%2F28%2FHexo%E5%8D%9A%E5%AE%A2%E4%B8%BB%E9%A2%98-NexT%2F</url>
    <content type="text"><![CDATA[Hexo博客提供了主题可供选择，我的主题是NexT-V5.1.4，简约黑白风格。 首先说明一下站点配置文件_config.yml和主题配置文件_config.yml 配置文件 目录 站点配置文件_config.yml blog 主题配置文件_config.yml blog/themes/next [注]blog是我的Hexo博客文件夹，文件夹下面有node_modules，source，themes等文件夹 安装NexT主题1234# 在blog目录下，打开cmdgit clone --branch v5.1.4 https://github.com/iissnan/hexo-theme-next themes/next# 打开站点配置文件_config.yml，找到themetheme: next Ok，完成！ 选择主题样式123456# 打开主题配置文件_config.yml，找到scheme，自行选择一种样式# Schemes#scheme: Muse#scheme: Mistscheme: Pisces#scheme: Gemini 添加RSS功能 执行命令，安装插件 1npm install hexo-generator-feed –save 编辑主题配置文件_config.yml， rss修改为如下 1rss: /atom.xml 添加标签页面 新建页面 在终端窗口下，定位到 Hexo 站点目录下。使用 hexo new page 新建一个页面，命名为 tags ：12cd your-hexo-sitehexo new page tags 设置页面类型 编辑刚新建的页面，将页面的类型设置为 tags ，主题将自动为这个页面显示标签云。页面内容如下：12345title: 标签date: 2014-12-22 12:39:04type: &quot;tags&quot;comments: false--- 注意：如果有集成评论服务，页面也会带有评论。 若需要关闭的话，请添加字段 comments 并将值设置为 false，如： 修改菜单 在菜单中添加链接。编辑主题配置文件_config.yml，添加 tags 到 menu 中，如下: 123456789menu: home: / || home #about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat 添加分类页面 新建页面 在终端窗口下，定位到 Hexo 站点目录下。使用 hexo new page 新建一个页面，命名为 categories ：12cd your-hexo-sitehexo new page categories 设置页面类型 编辑刚新建的页面，将页面的 type 设置为 categories ，主题将自动为这个页面显示分类。页面内容如下：12345title: 分类date: 2014-12-22 12:39:04type: &quot;categories&quot;comments: false--- 注意：如果有集成评论服务，页面也会带有评论。 若需要关闭的话，请添加字段 comments 并将值设置为 false，如： 修改菜单 在菜单中添加链接。编辑主题配置文件_config.yml，添加 categories 到 menu 中，如下: 123456789menu: home: / || home #about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat 添加sitemap站点地图开启打赏功能越来越多的平台（微信公众平台，新浪微博，简书，百度打赏等）支持打赏功能，付费阅读时代越来越近，特此增加了打赏功能，支持微信打赏和支付宝打赏。只需要主题配置文件_config.yml 中填入微信和支付宝收款二维码图片地址 即可开启该功能。 123reward_comment: 坚持原创技术分享，您的支持将鼓励我继续创作！wechatpay: /path/to/wechat-reward-imagealipay: /path/to/alipay-reward-image 数学公式支持123# MathJax Supportmathjax: enable: true 腾讯分析请登录腾讯分析，登录并获取分析的ID。 然后在主题配置文件_config.yml里将ID放置tencent_analytics字段。 不蒜子统计编辑主题配置文件_config.yml中的busuanzi_count的配置项1234567891011121314151617# Show PV/UV of the website/page with busuanzi.# Get more information on http://ibruce.info/2015/04/04/busuanzi/busuanzi_count: # 开启全局开关enable: true enable: true # 站点UV配置 site_uv: true site_uv_header: &lt;i class=&quot;fa fa-user&quot;&gt;&lt;/i&gt;本站访客数 site_uv_footer: 人次 # 站点PV配置 site_pv: true site_pv_header: &lt;i class=&quot;fa fa-eye&quot;&gt;&lt;/i&gt;本站总访问量 site_pv_footer: 次 # 单页面PVp配置，这里我没有启用这个功能，我使用了leancloud的文章阅读次数统计 page_pv: false page_pv_header: &lt;i class=&quot;fa fa-file-o&quot;&gt;&lt;/i&gt; page_pv_footer: 文章阅读量统计打开LeanCloud官网，进行注册并登录。 创建应用，命名为blog 打开blog 新建Class，命名为Counter 点击应用key，获取App ID和App Key 打开主题配置文件_config.yml，找到leancloud_visitors:，填入你的App ID和App Key 123456# Show number of visitors to each article.# You can visit https://leancloud.cn get AppID and AppKey.leancloud_visitors: enable: true app_id: app_key: # 添加来比力评论 优点：配置简单 登陆来必力获取你的 LiveRe UID。 编辑主题配置文件_config.yml，编辑 livere_uid字段，设置如下： 1livere_uid: #your livere_uid 添加valine评论如果对hypercomments不满意，可以试一下valine。valine是LeanCloud的一款极简的评论系统，并且还支持markdown功能！ 打开LeanCloud官网，登录。 创建应用，命名为comment 打开comment 点击应用key，获取App ID和App Key 打开主题配置文件_config.yml，找到valine，填入你的App ID和App Key 12345678910111213# Valine.# You can get your appid and appkey from https://leancloud.cn# more info please open https://valine.js.orgvaline: enable: true appid: appkey: notify: false # mail notifier , https://github.com/xCss/Valine/wiki verify: false # Verification code placeholder: Just go go # comment box placeholder avatar: mm # gravatar style guest_info: nick,mail,link # custom comment header pageSize: 10 # pagination size 主题页脚 由Hexo强力驱动 | 主题 - NexT.Pisces 打开主题配置文件_config.yml，找到copyright，powered: false 12345678910111213footer: # Specify the date when the site was setup. # If not defined, current year will be used. #since: 2015 # Icon between year and copyright info. icon: user # If not defined, will be used `author` from Hexo main config. copyright: # ------------------------------------------------------------- # Hexo link (Powered by Hexo). powered: false 效果如下 主题 — NexT.Pisces v5.1.4 添加版权信息在目录 next/layout/_macro/下添加 my-copyright.swig： 123456789101112131415161718192021222324252627282930&#123;% if page.copyright %&#125;&lt;div class=&quot;my_post_copyright&quot;&gt; &lt;script src=&quot;//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js&quot;&gt;&lt;/script&gt; &lt;!-- JS库 sweetalert 可修改路径 --&gt; &lt;script src=&quot;https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://unpkg.com/sweetalert/dist/sweetalert.min.js&quot;&gt;&lt;/script&gt; &lt;p&gt;&lt;span&gt;本文标题:&lt;/span&gt;&lt;a href=&quot;&#123;&#123; url_for(page.path) &#125;&#125;&quot;&gt;&#123;&#123; page.title &#125;&#125;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;文章作者:&lt;/span&gt;&lt;a href=&quot;/&quot; title=&quot;访问 &#123;&#123; theme.author &#125;&#125; 的个人博客&quot;&gt;&#123;&#123; theme.author &#125;&#125;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;发布时间:&lt;/span&gt;&#123;&#123; page.date.format(&quot;YYYY年MM月DD日 - HH:MM&quot;) &#125;&#125;&lt;/p&gt; &lt;p&gt;&lt;span&gt;最后更新:&lt;/span&gt;&#123;&#123; page.updated.format(&quot;YYYY年MM月DD日 - HH:MM&quot;) &#125;&#125;&lt;/p&gt; &lt;p&gt;&lt;span&gt;原始链接:&lt;/span&gt;&lt;a href=&quot;&#123;&#123; url_for(page.path) &#125;&#125;&quot; title=&quot;&#123;&#123; page.title &#125;&#125;&quot;&gt;&#123;&#123; page.permalink &#125;&#125;&lt;/a&gt; &lt;span class=&quot;copy-path&quot; title=&quot;点击复制文章链接&quot;&gt;&lt;i class=&quot;fa fa-clipboard&quot; data-clipboard-text=&quot;&#123;&#123; page.permalink &#125;&#125;&quot; aria-label=&quot;复制成功！&quot;&gt;&lt;/i&gt;&lt;/span&gt; &lt;/p&gt; &lt;p&gt;&lt;span&gt;许可协议:&lt;/span&gt;&lt;i class=&quot;fa fa-creative-commons&quot;&gt;&lt;/i&gt; &lt;a rel=&quot;license&quot; href=&quot;https://creativecommons.org/licenses/by-nc-nd/4.0/&quot; target=&quot;_blank&quot; title=&quot;Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)&quot;&gt;署名-非商业性使用-禁止演绎 4.0 国际&lt;/a&gt; 转载请保留原文链接及作者。&lt;/p&gt; &lt;/div&gt;&lt;script&gt; var clipboard = new Clipboard(&apos;.fa-clipboard&apos;); $(&quot;.fa-clipboard&quot;).click(function()&#123; clipboard.on(&apos;success&apos;, function()&#123; swal(&#123; title: &quot;&quot;, text: &apos;复制成功&apos;, icon: &quot;success&quot;, showConfirmButton: true &#125;); &#125;); &#125;); &lt;/script&gt;&#123;% endif %&#125; 在目录next/source/css/_common/components/post/下添加my-post-copyright.styl： 123456789101112131415161718192021222324252627282930313233343536373839404142434445.my_post_copyright &#123; width: 85%; max-width: 45em; margin: 2.8em auto 0; padding: 0.5em 1.0em; border: 1px solid #d3d3d3; font-size: 0.93rem; line-height: 1.6em; word-break: break-all; background: rgba(255,255,255,0.4);&#125;.my_post_copyright p&#123;margin:0;&#125;.my_post_copyright span &#123; display: inline-block; width: 5.2em; color: #b5b5b5; font-weight: bold;&#125;.my_post_copyright .raw &#123; margin-left: 1em; width: 5em;&#125;.my_post_copyright a &#123; color: #808080; border-bottom:0;&#125;.my_post_copyright a:hover &#123; color: #a3d2a3; text-decoration: underline;&#125;.my_post_copyright:hover .fa-clipboard &#123; color: #000;&#125;.my_post_copyright .post-url:hover &#123; font-weight: normal;&#125;.my_post_copyright .copy-path &#123; margin-left: 1em; width: 1em; +mobile()&#123;display:none;&#125;&#125;.my_post_copyright .copy-path:hover &#123; color: #808080; cursor: pointer;&#125; 修改next/layout/_macro/post.swig，在代码 12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include &apos;wechat-subscriber.swig&apos; %&#125; &#123;% endif %&#125;&lt;/div&gt; 之前添加增加如下代码： 12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include &apos;my-copyright.swig&apos; %&#125; &#123;% endif %&#125;&lt;/div&gt; 修改next/source/css/_common/components/post/post.styl文件，在最后一行增加代码： 1@import &quot;my-post-copyright&quot; 百度分享编辑主题配置文件_config.yml，找到baidushare 123456# Baidu Share# Available value:# button | slide# Warning: Baidu Share does not support https.baidushare: type: button 搜索功能 安装 hexo-generator-searchdb，在站点的根目录下执行以下命令： 1npm install hexo-generator-searchdb --save 编辑站点配置文件_config.yml，新增以下内容到任意位置： 12345search: path: search.xml field: post format: html limit: 10000 编辑主题配置文件_config.yml，启用本地搜索功能： 123# Local searchlocal_search: enable: true 修改文章底部的那个带#号的标签修改模板/themes/next/layout/_macro/post.swig，搜索 rel=”tag”&gt;#，将 # 换成 Pisces文章分割线长度修改blog\themes\next\source\css_common\components\post文件夹下的post-eof.styl 12345678910.posts-expand &#123; .post-eof &#123; display: block; margin: $post-eof-margin-top auto $post-eof-margin-bottom; width: 8%; # 分割线长度 height: 1px; background: $grey-light; text-align: center; &#125;&#125; 博文压缩在blog/目录下 执行一下命令： 12npm install gulp -gnpm install gulp-minify-css gulp-uglify gulp-htmlmin gulp-htmlclean gulp --save 新建gulpfile.js，并填入一下内容 123456789101112131415161718192021222324252627282930313233var gulp = require(&apos;gulp&apos;);var minifycss = require(&apos;gulp-minify-css&apos;);var uglify = require(&apos;gulp-uglify&apos;);var htmlmin = require(&apos;gulp-htmlmin&apos;);var htmlclean = require(&apos;gulp-htmlclean&apos;);// 压缩 public 目录 cssgulp.task(&apos;minify-css&apos;, function() &#123; return gulp.src(&apos;./public/**/*.css&apos;) .pipe(minifycss()) .pipe(gulp.dest(&apos;./public&apos;));&#125;);// 压缩 public 目录 htmlgulp.task(&apos;minify-html&apos;, function() &#123; return gulp.src(&apos;./public/**/*.html&apos;) .pipe(htmlclean()) .pipe(htmlmin(&#123; removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, &#125;)) .pipe(gulp.dest(&apos;./public&apos;))&#125;);// 压缩 public/js 目录 jsgulp.task(&apos;minify-js&apos;, function() &#123; return gulp.src(&apos;./public/**/*.js&apos;) .pipe(uglify()) .pipe(gulp.dest(&apos;./public&apos;));&#125;);// 执行 gulp 命令时执行的任务gulp.task(&apos;default&apos;, [ &apos;minify-html&apos;,&apos;minify-css&apos;,&apos;minify-js&apos;]); 执行hexo g &amp;&amp; gulp就会根据 gulpfile.js 中的配置，对 public 目录中的静态资源文件进行压缩 参考文献 GitHub+Hexo 搭建个人网站详细教程 安装Next主题 第三方集成服务 为NexT主题添加文章阅读量统计功能 为你的Hexo加上评论系统-Valine 主题配置 Blog + SEO hexo的next主题个性化教程:打造炫酷网站]]></content>
      <categories>
        <category>博客搭建</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>NexT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Git+Node.js+GitHub Pages+Coding Pages搭建博客]]></title>
    <url>%2F2018%2F04%2F27%2FHexo-Git-Node-js-GitHub-Pages-Coding-Pages%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[利用Hexo+Git+Node.js+GitHub Pages+Coding Pages刚刚搭建完个人博客，记录一下搭建过程，方便以后查看，有任何问题都可以在下方评论区留言。 前期准备Hexo博客搭建完成后需要部署于GitHub Pages及Coding Pages上，所以先注册好GitHub及Coding的账号。 Git点击进入Git官网，点击Downloads，下载对应自己操作系统的Git版本，并进行安装。 测试Git是否安装成功 12# 打开cmdgit --version 配置Git用户信息 12git config --global user.name &quot;你的GitHub用户名&quot;git config --global user.email &quot;你的GitHub注册邮箱&quot; 生成ssh密钥文件 1ssh-keygen -t rsa -C &quot;你的GitHub注册邮箱&quot; 回车，Windows下可以在C:\Users\Administrator.ssh路径下找到.pub公钥 GitHub配置公钥进入你的GitHub，点击Settings\SSH and GPG keys，点击New SSH key，将刚才生成的.pub文件内容全部复制进Key栏中，Title栏可以相应取名，测试一下是否配置成功 1ssh git@github.com Coding配置公钥Coding配置公钥与GitHub差不多。测试一下是否配置成功 1ssh -T git@git.coding.net Node.js点击进入node官网，点击DOWNLOADS，下载对应自己操作系统的Node.js版本，并进行安装。 测试安装是否成功 123# 打开cmdnode -vnpm -v 更换npm安装源 1234# 淘宝源npm config set registry https://registry.npm.taobao.org# 打开cmd，测试是否更换成功npm config get registry Hexo 点击进入Hexo官网，查看Hexo安装教程。 1234567# 新建文件夹Blog，打开cmdcd Blognpm install hexo-cli -ghexo init blogcd bloghexo ghexo d Ok，恭喜你，hexo博客搭建成功！ 这是Hexo博客的初始主题landscape，可以更换为其他主题，其他Hexo指令可以到官网Getting Started查看。 同时部署到GitHub和Coding 修改站点配置文件_config.yml，关联GitHub及Coding 12345678# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: github: git@github.com:username/username.github.io.git coding: git@git.coding.net:username/username.git branch: master 安装git插件 1npm install hexo-deployer-git --save 测试一下是否可以部署成功 12hexo ghexo d 出现INFO Deploy done: git说明部署成功，可以打开GitHub及Coding相应repo查看！ NexT主题配置参考文献 GitHub+Hexo 搭建个人网站详细教程]]></content>
      <categories>
        <category>博客搭建</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Git</tag>
        <tag>Node.js</tag>
        <tag>GitHub</tag>
        <tag>Coding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Coding备份Hexo博客源文件]]></title>
    <url>%2F2018%2F04%2F24%2FCoding%E5%A4%87%E4%BB%BDHexo%E5%8D%9A%E5%AE%A2%E6%BA%90%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Github Pages + Hexo搭建个人博客后，使用以下hexo指令可以将静态博客的内容部署到Github上。 12hexo ghexo d 但是博客的源文件只能放在本地电脑上，这样如果我们本地电脑出了问题，在新的电脑上怎么恢复我们的博客源文件呢？上网查了很多方法，发现好复杂，这里按参考文献[1]中方法，使用国内的代码托管平台——Coding，将我们的Hexo博客源文件备份到Coding上的私有repo上，这样更换电脑后我们也不需要重新搭建我们的博客了。 创建Coding私有repo在Coding上创建私有repo，命名为Blog，Coding上支持私有repo。 上传博客源文件备份 这里如参考文献[1]中指出的一样，需要删掉博客源文件根目录下及theme/next下的.git文件夹，否则上传的备份无法还原我们的博客。 删掉博客源文件根目录下及theme/next下的.git文件夹 在Blog目录下打开cmd [注]我的博客的目录结构为Blog/blog，其中blog是我的博客文件夹。 12345git initgit add .git commit -m &quot;Initial Commit&quot;git remote add origin https://git.coding.net/username/Blog.gitgit push -u origin master OK，备份完成。 新设备上恢复博客源文件将Coding上的repo，即Blog，克隆到本地。 1git clone https://git.coding.net/username/Blog.git 使用npm指令安装Hexo12cd blognpm install hexo --save 测试一下 123hexo ghexo shexo d Ok，恢复完成！新手上路，如有错误，还望指出，谢谢！ 参考文献[1] 使用Git备份博客工程文件]]></content>
      <categories>
        <category>博客搭建</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Git</tag>
        <tag>Coding</tag>
      </tags>
  </entry>
</search>
